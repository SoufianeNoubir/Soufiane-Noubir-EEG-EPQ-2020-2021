{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spatial Imagery.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSiknNpv3PQqPsZLmIYpa/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoufianeNoubir/Soufiane-Noubir-EEG-EPQ-2020-2021/blob/main/Spatial_Imagery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qc8Hhq4Lg_za",
        "outputId": "dc97980f-74a3-4147-dd27-54a1370f0872"
      },
      "source": [
        "pip install mne\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mne in /usr/local/lib/python3.7/dist-packages (0.22.0)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from mne) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from mne) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MDCbs8bN0Fq_",
        "outputId": "10c8b4a2-1c89-4364-e1da-4ca6ec6ce607"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7uCj7HOTgPpF",
        "outputId": "06a6dead-8eec-480e-d4c5-d959121c2634"
      },
      "source": [
        "cd /content/drive/My Drive/EPQ datasets/BCICIV_2a_gdf.zip (Unzipped Files)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/EPQ datasets/BCICIV_2a_gdf.zip (Unzipped Files)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8aGZjeuZgP3p",
        "outputId": "e358cf76-9a44-4527-cfe5-1de0a6ab4451"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/EPQ datasets/BCICIV_2a_gdf.zip (Unzipped Files)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeJ_bn6igdLg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "37ccb3af-4cee-4b5b-825c-8add11d1129b"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import mne\n",
        "from scipy.linalg import eigh\n",
        "import math\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "#reading in file\n",
        "raw=mne.io.read_raw_gdf('A01T.gdf')\n",
        "#getting the data\n",
        "data=raw.get_data()\n",
        "#getting the labels\n",
        "events_A, event_id_A = mne.events_from_annotations(raw)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting EDF parameters from /content/drive/My Drive/EPQ datasets/BCICIV_2a_gdf.zip (Unzipped Files)/A01T.gdf...\n",
            "GDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mne/io/edf/edf.py:1044: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
            "  etmode = np.fromstring(etmode, UINT8).tolist()[0]\n",
            "<ipython-input-49-0d7db38feb49>:12: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
            "  raw=mne.io.read_raw_gdf('A01T.gdf')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir3X-rG6BFOd"
      },
      "source": [
        "#splitting the dataset up by label\n",
        "rejections=[]\n",
        "left=[]\n",
        "right=[]\n",
        "up=[]\n",
        "down=[]\n",
        "\n",
        "for i in events_A:\n",
        "  if i[2]==1:\n",
        "    if i[0]%2==0:\n",
        "      rejections.append((i[0])/2)\n",
        "    else:\n",
        "      rejections.append(((i[0])+1)/2)\n",
        "  if i[2]==7:\n",
        "    if i[0]%2==0:\n",
        "      left.append((i[0])/2)\n",
        "    else:\n",
        "      left.append(((i[0])+1)/2)\n",
        "  if i[2]==8:\n",
        "    if i[0]%2==0:\n",
        "      right.append((i[0])/2)\n",
        "    else:\n",
        "      right.append(((i[0])+1)/2)\n",
        "  if i[2]==10:\n",
        "    if i[0]%2==0:\n",
        "      up.append((i[0])/2)\n",
        "    else:\n",
        "      up.append(((i[0])+1)/2)\n",
        "  if i[2]==9:\n",
        "    if i[0]%2==0:\n",
        "      down.append((i[0])/2)\n",
        "    else:\n",
        "      down.append(((i[0])+1)/2)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz9pY--wJv4S"
      },
      "source": [
        "#removes unwanted electrodes\n",
        "def remove_electrodes(data,numbers):\n",
        "  output=[]\n",
        "  for i in range(len(data)):\n",
        "    if i in numbers:\n",
        "      \n",
        "      output.append(data[i])\n",
        "  \n",
        "  return output\n",
        "data=remove_electrodes(data,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxLrJoa3NFG8"
      },
      "source": [
        "#downsamples data by a given factor\n",
        "def downsample(data,factor):\n",
        "  output=[]\n",
        "  current=[]\n",
        "  for i in data:\n",
        "    for j in range(len(i)):\n",
        "      if j%factor==0:\n",
        "        current.append(i[j])\n",
        "    output.append(current)\n",
        "    current=[]\n",
        "  return output\n",
        "data_downsample=downsample(data,2)\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLz-70QcNI3-"
      },
      "source": [
        "#splits data up into segments of EEG data corresponding to one of the 4 classes\n",
        "up_data=[]\n",
        "down_data=[]\n",
        "left_data=[]\n",
        "right_data=[]\n",
        "current=[]\n",
        "for i in up:\n",
        "  if (int(i)-250) not in rejections:\n",
        "    for j in data_downsample:\n",
        "      current.append(j[int(i)+125:int(i)+500])\n",
        "    up_data.append(current)\n",
        "    current=[]\n",
        "for i in down:\n",
        "  if (int(i)-250) not in rejections:\n",
        "    for j in data_downsample:\n",
        "      current.append(j[int(i)+125:int(i)+500])\n",
        "    down_data.append(current)\n",
        "    current=[]\n",
        "for i in right:\n",
        "  if (int(i)-250) not in rejections:\n",
        "    for j in data_downsample:\n",
        "      current.append(j[int(i)+125:int(i)+500])\n",
        "    right_data.append(current)\n",
        "    current=[]\n",
        "for i in left:\n",
        "  if (int(i)-250) not in rejections:\n",
        "    for j in data_downsample:\n",
        "      current.append(j[int(i)+125:int(i)+500])\n",
        "    left_data.append(current)\n",
        "    current=[]"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xucARZbFNIrO"
      },
      "source": [
        "#splitting of dataset into training and testing data\n",
        "left_data_evaluation=[]\n",
        "right_data_evaluation=[]\n",
        "down_data_evaluation=[]\n",
        "up_data_evaluation=[]\n",
        "import random\n",
        "for i in up_data:\n",
        "  x=random.randint(1,4)\n",
        "  if x==4:\n",
        "    up_data.remove(i)\n",
        "    up_data_evaluation.append(i)\n",
        "for i in right_data:\n",
        "  x=random.randint(1,4)\n",
        "  if x==4:\n",
        "    right_data.remove(i)\n",
        "    right_data_evaluation.append(i)\n",
        "for i in down_data:\n",
        "  x=random.randint(1,4)\n",
        "  if x==4:\n",
        "    down_data.remove(i)\n",
        "    down_data_evaluation.append(i)\n",
        "for i in left_data:\n",
        "  x=random.randint(1,4)\n",
        "  if x==4:\n",
        "    left_data.remove(i)\n",
        "    left_data_evaluation.append(i)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lP_CGQR8EKw"
      },
      "source": [
        "space_data=[]\n",
        "space_data.append(up_data)\n",
        "space_data.append(right_data)\n",
        "space_data.append(down_data)\n",
        "space_data.append(left_data)\n",
        "space_data_evaluation=[]\n",
        "space_data_evaluation.append(up_data_evaluation)\n",
        "space_data_evaluation.append(right_data_evaluation)\n",
        "space_data_evaluation.append(down_data_evaluation)\n",
        "space_data_evaluation.append(left_data_evaluation)\n",
        "x_all_test=[]\n",
        "y_all_test=[]\n",
        "x_all_test+=up_data_evaluation\n",
        "x_all_test+=right_data_evaluation\n",
        "x_all_test+=down_data_evaluation\n",
        "x_all_test+=left_data_evaluation\n",
        "for i in up_data_evaluation:\n",
        "  y_all_test.append(1)\n",
        "for i in right_data_evaluation:\n",
        "  y_all_test.append(2)\n",
        "for i in down_data_evaluation:\n",
        "  y_all_test.append(3)\n",
        "for i in left_data_evaluation:\n",
        "  y_all_test.append(4)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j024jS_NdmD"
      },
      "source": [
        "#implementation of standard CSP approach\n",
        "def csp(data_1,data_2):\n",
        "  mean_cov_1=np.cov(np.array(data_1[0]))\n",
        "  \n",
        "  mean_cov_2=np.cov(np.array(data_2[0]))\n",
        " \n",
        "  for i in data_1[1:]:\n",
        "    mean_cov_1+=np.cov(np.array(i))\n",
        "    \n",
        "  for i in data_2[1:]:\n",
        "    mean_cov_2+=np.cov(np.array(i))\n",
        "    \n",
        "  mean_cov_1=mean_cov_1/len(data_1)\n",
        "  mean_cov_2=mean_cov_2/len(data_2)\n",
        "\n",
        "  \n",
        "  w_1, v_1 = np.linalg.eig(np.dot(np.linalg.inv(mean_cov_2+mean_cov_1),mean_cov_1))\n",
        "  sort=np.argsort(w_1)\n",
        "  \n",
        "  \n",
        "  return ([v_1[sort[0]],v_1[sort[1]],v_1[sort[2]],v_1[sort[-3]],v_1[sort[-2]],v_1[sort[-1]]])"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zEnsT3jjJIT"
      },
      "source": [
        "#implementation of multiclass CSP\n",
        "def multiclass_csp(data):\n",
        "  total_elements=0\n",
        "  for i in data:\n",
        "    total_elements+=len(i)\n",
        "  average=[]\n",
        "  for i in data:\n",
        "    total=np.array(i[0])\n",
        "    \n",
        "    for j in i[1:]:\n",
        "      \n",
        "      total+=np.array(j)\n",
        "    total=total/len(i)\n",
        "    \n",
        "      \n",
        "    average.append(total)\n",
        "  total_average=len(data[0])*average[0]\n",
        "  for i in range(len(data[1:])):\n",
        "    total_average+=len(data[i])*average[i]\n",
        "  total_average=total_average/total_elements\n",
        "  within_class=np.dot(data[0][0]-average[0],(data[0][0]-average[0]).transpose())\n",
        "  for i in range(len(data)):\n",
        "    \n",
        "    for j in data[i]:\n",
        "      within_class+=np.dot(j-average[i],(j-average[i]).transpose())\n",
        "  within_class-=np.dot(data[0][0]-average[0],(data[0][0]-average[0]).transpose())\n",
        "  within_class=within_class/total_elements\n",
        "  between_class=len(data[0])*(np.dot(average[0]-total_average,(average[0]-total_average).transpose()))\n",
        "  for i in range(len(average[1:])):\n",
        "    between_class+=len(data[i])*(np.dot(average[i]-total_average,(average[i]-total_average).transpose()))\n",
        "  between_class=between_class/len(data)\n",
        "  w_1, v_1 = np.linalg.eig(np.dot(np.linalg.inv(between_class+within_class),between_class))\n",
        "  sort=np.argsort(w_1)\n",
        "  return ([v_1[sort[0]],v_1[sort[1]],v_1[sort[2]],v_1[sort[3]],v_1[sort[4]],v_1[sort[5]]])\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2El8eEQV5uzp"
      },
      "source": [
        "# implementation of feature extraction using filters generate from CSP\n",
        "def feature_extraction(up_data,filters):\n",
        "  features=[]\n",
        "  for i in filters:\n",
        "    features.append(math.log(np.var(np.dot(i.transpose(),up_data))))\n",
        "    \n",
        "  return features"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOkdDMs850Or"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWr31f8v57tV"
      },
      "source": [
        "csp_filters_left=csp(left_data,right_data+up_data+down_data)\n",
        "csp_filters_right=csp(right_data,left_data+up_data+down_data)\n",
        "csp_filters_down=csp(down_data,right_data+up_data+left_data)\n",
        "csp_filters_up=csp(up_data,right_data+left_data+down_data)\n",
        "csp_filters_up_down=csp(up_data,down_data)\n",
        "csp_filters_down_right=csp(right_data,down_data)\n",
        "csp_filters_up_right=csp(up_data,right_data)\n",
        "csp_filters_down_left=csp(left_data,down_data)\n",
        "csp_filters_right_left=csp(right_data,left_data)\n",
        "csp_filters_up_left=csp(up_data,left_data)\n",
        "csp_filters_v_h=csp(up_data+down_data,right_data+left_data)\n",
        "csp_multiclass=multiclass_csp(space_data)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er5jCLXQHAW6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn5cdG7Z58JL"
      },
      "source": [
        "x_vertical_horizontal_train=[]\n",
        "y_vertical_horizontal_train=[]\n",
        "x_vertical_horizontal_test=[]\n",
        "y_vertical_horizontal_test=[]\n",
        "x_up_down_train=[]\n",
        "y_up_down_train=[]\n",
        "x_up_down_test=[]\n",
        "y_up_down_test=[]\n",
        "x_left_right_train=[]\n",
        "y_left_right_train=[]\n",
        "x_left_right_test=[]\n",
        "y_left_right_test=[]\n",
        "x_left_train=[]\n",
        "y_left_train=[]\n",
        "x_left_test=[]\n",
        "y_left_test=[]\n",
        "x_right_train=[]\n",
        "y_right_train=[]\n",
        "x_right_test=[]\n",
        "y_right_test=[]\n",
        "x_up_train=[]\n",
        "y_up_train=[]\n",
        "x_up_test=[]\n",
        "y_up_test=[]\n",
        "x_down_train=[]\n",
        "y_down_train=[]\n",
        "x_down_test=[]\n",
        "y_down_test=[]\n",
        "x_right_up_train=[]\n",
        "y_right_up_train=[]\n",
        "x_right_up_test=[]\n",
        "y_right_up_test=[]\n",
        "x_right_down_train=[]\n",
        "y_right_down_train=[]\n",
        "x_right_down_test=[]\n",
        "y_right_down_test=[]\n",
        "x_left_up_train=[]\n",
        "y_left_up_train=[]\n",
        "x_left_up_test=[]\n",
        "y_left_up_test=[]\n",
        "x_left_down_train=[]\n",
        "y_left_down_train=[]\n",
        "x_left_down_test=[]\n",
        "y_left_down_test=[]\n",
        "x_csp_space_train=[]\n",
        "y_csp_space_train=[]\n",
        "x_csp_space_test=[]\n",
        "y_csp_space_test=[]\n",
        "x_train_nn_space=[]\n",
        "y_train_nn_space=[]\n",
        "x_test_nn_space=[]\n",
        "y_test_nn_space=[]"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oewp1Mgo59Gv"
      },
      "source": [
        "for i in up_data:\n",
        "  x_vertical_horizontal_train.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in down_data:\n",
        "  x_vertical_horizontal_train.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in right_data:\n",
        "  x_vertical_horizontal_train.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in left_data:\n",
        "  x_vertical_horizontal_train.append(feature_extraction(i,csp_filters_v_h))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_vertical_horizontal_test.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in down_data_evaluation:\n",
        "  x_vertical_horizontal_test.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in right_data_evaluation:\n",
        "  x_vertical_horizontal_test.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in left_data_evaluation:\n",
        "  x_vertical_horizontal_test.append(feature_extraction(i,csp_filters_v_h))\n",
        "\n",
        "for i in up_data:\n",
        "  x_up_down_train.append(feature_extraction(i,csp_filters_up_down))\n",
        "for i in down_data:\n",
        "  x_up_down_train.append(feature_extraction(i,csp_filters_up_down))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_up_down_test.append(feature_extraction(i,csp_filters_up_down))\n",
        "for i in down_data_evaluation:\n",
        "  x_up_down_test.append(feature_extraction(i,csp_filters_up_down))\n",
        "\n",
        "for i in right_data:\n",
        "  x_left_right_train.append(feature_extraction(i,csp_filters_right_left))\n",
        "for i in left_data:\n",
        "  x_left_right_train.append(feature_extraction(i,csp_filters_right_left))\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  x_left_right_test.append(feature_extraction(i,csp_filters_right_left))\n",
        "for i in left_data_evaluation:\n",
        "  x_left_right_test.append(feature_extraction(i,csp_filters_right_left))\n",
        "\n",
        "for i in right_data:\n",
        "  x_right_up_train.append(feature_extraction(i,csp_filters_up_right))\n",
        "for i in up_data:\n",
        "  x_right_up_train.append(feature_extraction(i,csp_filters_up_right))\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  x_right_up_test.append(feature_extraction(i,csp_filters_up_right))\n",
        "for i in up_data_evaluation:\n",
        "  x_right_up_test.append(feature_extraction(i,csp_filters_up_right))\n",
        "\n",
        "for i in right_data:\n",
        "  x_right_down_train.append(feature_extraction(i,csp_filters_down_right))\n",
        "for i in down_data:\n",
        "  x_right_down_train.append(feature_extraction(i,csp_filters_down_right))\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  x_right_down_test.append(feature_extraction(i,csp_filters_down_right))\n",
        "for i in down_data_evaluation:\n",
        "  x_right_down_test.append(feature_extraction(i,csp_filters_down_right))\n",
        "\n",
        "for i in left_data:\n",
        "  x_left_up_train.append(feature_extraction(i,csp_filters_up_left))\n",
        "for i in up_data:\n",
        "  x_left_up_train.append(feature_extraction(i,csp_filters_up_left))\n",
        "\n",
        "for i in left_data_evaluation:\n",
        "  x_left_up_test.append(feature_extraction(i,csp_filters_up_left))\n",
        "for i in up_data_evaluation:\n",
        "  x_left_up_test.append(feature_extraction(i,csp_filters_up_left))\n",
        "\n",
        "\n",
        "for i in left_data:\n",
        "  x_left_down_train.append(feature_extraction(i,csp_filters_down_left))\n",
        "for i in down_data:\n",
        "  x_left_down_train.append(feature_extraction(i,csp_filters_down_left))\n",
        "\n",
        "for i in left_data_evaluation:\n",
        "  x_left_down_test.append(feature_extraction(i,csp_filters_down_left))\n",
        "for i in down_data_evaluation:\n",
        "  x_left_down_test.append(feature_extraction(i,csp_filters_down_left))\n",
        "\n",
        "\n",
        "for i in up_data:\n",
        "  x_left_train.append(feature_extraction(i,csp_filters_left))\n",
        "for i in down_data:\n",
        "  x_left_train.append(feature_extraction(i,csp_filters_left))\n",
        "for i in right_data:\n",
        "  x_left_train.append(feature_extraction(i,csp_filters_left))\n",
        "for i in left_data:\n",
        "  x_left_train.append(feature_extraction(i,csp_filters_left))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_left_test.append(feature_extraction(i,csp_filters_left))\n",
        "for i in down_data_evaluation:\n",
        "  x_left_test.append(feature_extraction(i,csp_filters_left))\n",
        "for i in right_data_evaluation:\n",
        "  x_left_test.append(feature_extraction(i,csp_filters_left))\n",
        "for i in left_data_evaluation:\n",
        "  x_left_test.append(feature_extraction(i,csp_filters_left))\n",
        "\n",
        "for i in up_data:\n",
        "  x_right_train.append(feature_extraction(i,csp_filters_right))\n",
        "for i in down_data:\n",
        "  x_right_train.append(feature_extraction(i,csp_filters_right))\n",
        "for i in right_data:\n",
        "  x_right_train.append(feature_extraction(i,csp_filters_right))\n",
        "for i in left_data:\n",
        "  x_right_train.append(feature_extraction(i,csp_filters_right))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_right_test.append(feature_extraction(i,csp_filters_right))\n",
        "for i in down_data_evaluation:\n",
        "  x_right_test.append(feature_extraction(i,csp_filters_right))\n",
        "for i in right_data_evaluation:\n",
        "  x_right_test.append(feature_extraction(i,csp_filters_right))\n",
        "for i in left_data_evaluation:\n",
        "  x_right_test.append(feature_extraction(i,csp_filters_right))\n",
        "\n",
        "for i in up_data:\n",
        "  x_up_train.append(feature_extraction(i,csp_filters_up))\n",
        "for i in down_data:\n",
        "  x_up_train.append(feature_extraction(i,csp_filters_up))\n",
        "for i in right_data:\n",
        "  x_up_train.append(feature_extraction(i,csp_filters_up))\n",
        "for i in left_data:\n",
        "  x_up_train.append(feature_extraction(i,csp_filters_up))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_up_test.append(feature_extraction(i,csp_filters_up))\n",
        "for i in down_data_evaluation:\n",
        "  x_up_test.append(feature_extraction(i,csp_filters_up))\n",
        "for i in right_data_evaluation:\n",
        "  x_up_test.append(feature_extraction(i,csp_filters_up))\n",
        "for i in left_data_evaluation:\n",
        "  x_up_test.append(feature_extraction(i,csp_filters_up))\n",
        "\n",
        "for i in up_data:\n",
        "  x_down_train.append(feature_extraction(i,csp_filters_down))\n",
        "for i in down_data:\n",
        "  x_down_train.append(feature_extraction(i,csp_filters_down))\n",
        "for i in right_data:\n",
        "  x_down_train.append(feature_extraction(i,csp_filters_down))\n",
        "for i in left_data:\n",
        "  x_down_train.append(feature_extraction(i,csp_filters_down))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_down_test.append(feature_extraction(i,csp_filters_down))\n",
        "for i in down_data_evaluation:\n",
        "  x_down_test.append(feature_extraction(i,csp_filters_down))\n",
        "for i in right_data_evaluation:\n",
        "  x_down_test.append(feature_extraction(i,csp_filters_down))\n",
        "for i in left_data_evaluation:\n",
        "  x_down_test.append(feature_extraction(i,csp_filters_down))\n",
        "\n",
        "for i in up_data:\n",
        "  x_csp_space_train.append(feature_extraction(i,csp_multiclass))\n",
        "for i in down_data:\n",
        "  x_csp_space_train.append(feature_extraction(i,csp_multiclass))\n",
        "for i in right_data:\n",
        "  x_csp_space_train.append(feature_extraction(i,csp_multiclass))\n",
        "for i in left_data:\n",
        "  x_csp_space_train.append(feature_extraction(i,csp_multiclass))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_csp_space_test.append(feature_extraction(i,csp_multiclass))\n",
        "for i in down_data_evaluation:\n",
        "  x_csp_space_test.append(feature_extraction(i,csp_multiclass))\n",
        "for i in right_data_evaluation:\n",
        "  x_csp_space_test.append(feature_extraction(i,csp_multiclass))\n",
        "for i in left_data_evaluation:\n",
        "  x_csp_space_test.append(feature_extraction(i,csp_multiclass))\n",
        "\n",
        "\n",
        "for i in space_data:\n",
        "  \n",
        "  for j in i:\n",
        "    intermediate=np.transpose(j)\n",
        "    intermediate_3=[]\n",
        "    for k in intermediate:\n",
        "      intermediate_2=[]\n",
        "      for l in k:\n",
        "        intermediate_2.append(np.array([l]))\n",
        "      intermediate_3.append(np.array(intermediate_2))\n",
        "    x_train_nn_space.append(np.array(intermediate_3))\n",
        "for i in up_data:\n",
        "  y_train_nn_space.append(np.array([1,0,0,0]))\n",
        "for i in right_data:\n",
        "  y_train_nn_space.append(np.array([0,1,0,0]))\n",
        "for i in down_data:\n",
        "  y_train_nn_space.append(np.array([0,0,1,0]))\n",
        "for i in left_data:\n",
        "  y_train_nn_space.append(np.array([0,0,0,1]))\n",
        "\n",
        "for i in space_data_evaluation:\n",
        "  \n",
        "  for j in i:\n",
        "    intermediate=np.transpose(j)\n",
        "    intermediate_3=[]\n",
        "    for k in intermediate:\n",
        "      intermediate_2=[]\n",
        "      for l in k:\n",
        "        intermediate_2.append(np.array([l]))\n",
        "      intermediate_3.append(np.array(intermediate_2))   \n",
        "    \n",
        "    x_test_nn_space.append(np.array(intermediate_3))\n",
        "    \n",
        "for i in up_data_evaluation:\n",
        "  y_test_nn_space.append(np.array([1,0,0,0]))\n",
        "for i in right_data_evaluation:\n",
        "  y_test_nn_space.append(np.array([0,1,0,0]))\n",
        "for i in down_data_evaluation:\n",
        "  y_test_nn_space.append(np.array([0,0,1,0]))\n",
        "for i in left_data_evaluation:\n",
        "  y_test_nn_space.append(np.array([0,0,0,1]))\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVpLOW9Q6AgE"
      },
      "source": [
        "for i in up_data:\n",
        " y_vertical_horizontal_train.append(1)\n",
        "for i in down_data:\n",
        "  y_vertical_horizontal_train.append(1)\n",
        "for i in right_data:\n",
        "  y_vertical_horizontal_train.append(2)\n",
        "for i in left_data:\n",
        "  y_vertical_horizontal_train.append(2)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_vertical_horizontal_test.append(1)\n",
        "for i in down_data_evaluation:\n",
        "  y_vertical_horizontal_test.append(1)\n",
        "for i in right_data_evaluation:\n",
        "  y_vertical_horizontal_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_vertical_horizontal_test.append(2)\n",
        "\n",
        "for i in up_data:\n",
        "  y_up_down_train.append(1)\n",
        "for i in down_data:\n",
        "  y_up_down_train.append(3)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_up_down_test.append(1)\n",
        "for i in down_data_evaluation:\n",
        "  y_up_down_test.append(3)\n",
        "\n",
        "for i in right_data:\n",
        "  y_left_right_train.append(2)\n",
        "for i in left_data:\n",
        "  y_left_right_train.append(4)\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  y_left_right_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_left_right_test.append(4)\n",
        "\n",
        "for i in right_data:\n",
        "  y_right_up_train.append(2)\n",
        "for i in up_data:\n",
        "  y_right_up_train.append(1)\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  y_right_up_test.append(2)\n",
        "for i in up_data_evaluation:\n",
        "  y_right_up_test.append(1)\n",
        "\n",
        "for i in right_data:\n",
        "  y_right_down_train.append(2)\n",
        "for i in down_data:\n",
        "  y_right_down_train.append(3)\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  y_right_down_test.append(2)\n",
        "for i in down_data_evaluation:\n",
        "  y_right_down_test.append(3)\n",
        "\n",
        "for i in left_data:\n",
        "  y_left_up_train.append(4)\n",
        "for i in up_data:\n",
        "  y_left_up_train.append(1)\n",
        "\n",
        "for i in left_data_evaluation:\n",
        "  y_left_up_test.append(4)\n",
        "for i in up_data_evaluation:\n",
        "  y_left_up_test.append(1)\n",
        "\n",
        "for i in left_data:\n",
        "  y_left_down_train.append(4)\n",
        "for i in down_data:\n",
        "  y_left_down_train.append(3)\n",
        "\n",
        "for i in left_data_evaluation:\n",
        "  y_left_down_test.append(4)\n",
        "for i in down_data_evaluation:\n",
        "  y_left_down_test.append(3)\n",
        "\n",
        "for i in up_data:\n",
        "  y_left_train.append(2)\n",
        "for i in down_data:\n",
        "  y_left_train.append(2)\n",
        "for i in right_data:\n",
        "  y_left_train.append(2)\n",
        "for i in left_data:\n",
        "  y_left_train.append(1)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_left_test.append(2)\n",
        "for i in down_data_evaluation:\n",
        "  y_left_test.append(2)\n",
        "for i in right_data_evaluation:\n",
        "  y_left_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_left_test.append(1)\n",
        "\n",
        "for i in up_data:\n",
        "  y_right_train.append(2)\n",
        "for i in down_data:\n",
        "  y_right_train.append(2)\n",
        "for i in right_data:\n",
        "  y_right_train.append(1)\n",
        "for i in left_data:\n",
        "  y_right_train.append(2)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_right_test.append(2)\n",
        "for i in down_data_evaluation:\n",
        "  y_right_test.append(2)\n",
        "for i in right_data_evaluation:\n",
        "  y_right_test.append(1)\n",
        "for i in left_data_evaluation:\n",
        "  y_right_test.append(2)\n",
        "\n",
        "for i in up_data:\n",
        "  y_up_train.append(1)\n",
        "for i in down_data:\n",
        "  y_up_train.append(2)\n",
        "for i in right_data:\n",
        "  y_up_train.append(2)\n",
        "for i in left_data:\n",
        "  y_up_train.append(2)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_up_test.append(1)\n",
        "for i in down_data_evaluation:\n",
        "  y_up_test.append(2)\n",
        "for i in right_data_evaluation:\n",
        "  y_up_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_up_test.append(2)\n",
        "\n",
        "for i in up_data:\n",
        "  y_down_train.append(2)\n",
        "for i in down_data:\n",
        "  y_down_train.append(1)\n",
        "for i in right_data:\n",
        "  y_down_train.append(2)\n",
        "for i in left_data:\n",
        "  y_down_train.append(2)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_down_test.append(2)\n",
        "for i in down_data_evaluation:\n",
        "  y_down_test.append(1)\n",
        "for i in right_data_evaluation:\n",
        "  y_down_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_down_test.append(2)\n",
        "\n",
        "for i in up_data:\n",
        "  y_csp_space_train.append(1)\n",
        "for i in down_data:\n",
        "  y_csp_space_train.append(3)\n",
        "for i in right_data:\n",
        "  y_csp_space_train.append(2)\n",
        "for i in left_data:\n",
        "  y_csp_space_train.append(4)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_csp_space_test.append(1)\n",
        "for i in down_data_evaluation:\n",
        "  y_csp_space_test.append(3)\n",
        "for i in right_data_evaluation:\n",
        "  y_csp_space_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_csp_space_test.append(4)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXE7ONAB6Gve"
      },
      "source": [
        "#uses one_v_one classification to output the predicted class\n",
        "def one_v_one(outputs):\n",
        "  num_1=outputs.count(1)\n",
        "  num_2=outputs.count(2)\n",
        "  num_3=outputs.count(3)\n",
        "  num_4=outputs.count(4)\n",
        "  num=[num_1,num_2,num_3,num_4]\n",
        "  if num.count(max(num))==1:\n",
        "    return (num.index(max(num)))+1\n",
        "  else:\n",
        "    \n",
        "    winners=[]\n",
        "    for i in range(len(num)):\n",
        "      if num[i] ==max(num):\n",
        "        winners.append(i+1)\n",
        "    if len(winners)==3:\n",
        "      return (\"alert\")\n",
        "    else:\n",
        "      if min(winners)==1:\n",
        "        return outputs[sum(winners)-3]\n",
        "      else:\n",
        "        return outputs[sum(winners)-2]\n",
        "#uses one_v_many classification to output the predicted class\n",
        "def one_v_many(probs):\n",
        "  return (probs.index(max(probs))+1)\n",
        "#uses binary search to output the predicted class\n",
        "def binary_search(outputs):\n",
        "  if outputs[0]==1:\n",
        "    return outputs[1]\n",
        "  else:\n",
        "    return outputs[2]\n",
        "#returns accuracy\n",
        "def accuracy(outputs,correct):\n",
        "  right=0\n",
        "  for i in range(len(outputs)):\n",
        "    if outputs[i]==correct[i]:\n",
        "      right+=1\n",
        "  return right/(len(outputs))"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUhTcyS46Mlp"
      },
      "source": [
        "multiclass_LDA = LinearDiscriminantAnalysis()\n",
        "multiclass_LDA.fit(x_csp_space_train,y_csp_space_train)\n",
        "print(multiclass_LDA.score(x_csp_space_test,y_csp_space_test))\n",
        "\n",
        "LDA_binary_search_v_h = LinearDiscriminantAnalysis()\n",
        "LDA_binary_search_v_h.fit(x_vertical_horizontal_train,y_vertical_horizontal_train)\n",
        "print(LDA_binary_search_v_h.score(x_vertical_horizontal_test,y_vertical_horizontal_test))\n",
        "\n",
        "LDA_ovo_u_d = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_u_d.fit(x_up_down_train,y_up_down_train)\n",
        "print(LDA_ovo_u_d.score(x_up_down_test,y_up_down_test))\n",
        "\n",
        "LDA_ovo_u_r = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_u_r .fit(x_right_up_train,y_right_up_train)\n",
        "print(LDA_ovo_u_r .score(x_right_up_test,y_right_up_test))\n",
        "\n",
        "LDA_ovo_u_l = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_u_l.fit(x_left_up_train,y_left_up_train)\n",
        "print(LDA_ovo_u_l.score(x_left_up_test,y_left_up_test))\n",
        "\n",
        "LDA_ovo_d_r = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_d_r.fit(x_right_down_train,y_right_down_train)\n",
        "print(LDA_ovo_d_r.score(x_right_down_test,y_right_down_test))\n",
        "\n",
        "LDA_ovo_d_l = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_d_l.fit(x_left_down_train,y_left_down_train)\n",
        "print(LDA_ovo_d_l.score(x_left_down_test,y_left_down_test))\n",
        "\n",
        "LDA_ovo_r_l = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_r_l.fit(x_left_right_train,y_left_right_train)\n",
        "print(LDA_ovo_r_l.score(x_left_right_test,y_left_right_test))\n",
        "\n",
        "LDA_ovm_u = LinearDiscriminantAnalysis()\n",
        "LDA_ovm_u.fit(x_up_train,y_up_train)\n",
        "print(LDA_ovm_u.score(x_up_test,y_up_test))\n",
        "\n",
        "LDA_ovm_r = LinearDiscriminantAnalysis()\n",
        "LDA_ovm_r.fit(x_right_train,y_right_trai76n)\n",
        "print(LDA_ovm_r.score(x_right_test,y_right_test))\n",
        "\n",
        "LDA_ovm_d = LinearDiscriminantAnalysis()\n",
        "LDA_ovm_d.fit(x_down_train,y_down_train)\n",
        "print(LDA_ovm_d.score(x_down_test,y_down_test))\n",
        "\n",
        "LDA_ovm_l = LinearDiscriminantAnalysis()\n",
        "LDA_ovm_l.fit(x_left_train,y_left_train)\n",
        "print(LDA_ovm_l.score(x_left_test,y_left_test))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbYtSFhDpoSs"
      },
      "source": [
        "multiclass_SVM = svm.SVC(probability=True)\n",
        "multiclass_SVM.fit(x_csp_space_train,y_csp_space_train)\n",
        "print(multiclass_SVM.score(x_csp_space_test,y_csp_space_test))\n",
        "\n",
        "SVM_binary_search_v_h = svm.SVC(probability=True)\n",
        "SVM_binary_search_v_h.fit(x_vertical_horizontal_train,y_vertical_horizontal_train)\n",
        "print(SVM_binary_search_v_h.score(x_vertical_horizontal_test,y_vertical_horizontal_test))\n",
        "\n",
        "SVM_ovo_u_d = svm.SVC(probability=True)\n",
        "SVM_ovo_u_d.fit(x_up_down_train,y_up_down_train)\n",
        "print(SVM_ovo_u_d.score(x_up_down_test,y_up_down_test))\n",
        "\n",
        "SVM_ovo_u_r = svm.SVC(probability=True)\n",
        "SVM_ovo_u_r .fit(x_right_up_train,y_right_up_train)\n",
        "print(SVM_ovo_u_r .score(x_right_up_test,y_right_up_test))\n",
        "\n",
        "SVM_ovo_u_l = svm.SVC(probability=True)\n",
        "SVM_ovo_u_l.fit(x_left_up_train,y_left_up_train)\n",
        "print(SVM_ovo_u_l.score(x_left_up_test,y_left_up_test))\n",
        "\n",
        "SVM_ovo_d_r = svm.SVC(probability=True)\n",
        "SVM_ovo_d_r.fit(x_right_down_train,y_right_down_train)\n",
        "print(SVM_ovo_d_r.score(x_right_down_test,y_right_down_test))\n",
        "\n",
        "SVM_ovo_d_l = svm.SVC(probability=True)\n",
        "SVM_ovo_d_l.fit(x_left_down_train,y_left_down_train)\n",
        "print(SVM_ovo_d_l.score(x_left_down_test,y_left_down_test))\n",
        "\n",
        "SVM_ovo_r_l = svm.SVC(probability=True)\n",
        "SVM_ovo_r_l.fit(x_left_right_train,y_left_right_train)\n",
        "print(SVM_ovo_r_l.score(x_left_right_test,y_left_right_test))\n",
        "\n",
        "SVM_ovm_u = svm.SVC(probability=True)\n",
        "SVM_ovm_u.fit(x_up_train,y_up_train)\n",
        "print(SVM_ovm_u.score(x_up_test,y_up_test))\n",
        "\n",
        "SVM_ovm_r = svm.SVC(probability=True)\n",
        "SVM_ovm_r.fit(x_right_train,y_right_train)\n",
        "print(SVM_ovm_r.score(x_right_test,y_right_test))\n",
        "\n",
        "SVM_ovm_d = svm.SVC(probability=True)\n",
        "SVM_ovm_d.fit(x_down_train,y_down_train)\n",
        "print(SVM_ovm_d.score(x_down_test,y_down_test))\n",
        "\n",
        "SVM_ovm_l = svm.SVC(probability=True)\n",
        "SVM_ovm_l.fit(x_left_train,y_left_train)\n",
        "print(SVM_ovm_l.score(x_left_test,y_left_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gweV-dhOppR9"
      },
      "source": [
        "multiclass_LR = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "multiclass_LR.fit(x_csp_space_train,y_csp_space_train)\n",
        "print(multiclass_LR.score(x_csp_space_test,y_csp_space_test))\n",
        "\n",
        "LR_binary_search_v_h = LogisticRegression()\n",
        "LR_binary_search_v_h.fit(x_vertical_horizontal_train,y_vertical_horizontal_train)\n",
        "print(LR_binary_search_v_h.score(x_vertical_horizontal_test,y_vertical_horizontal_test))\n",
        "\n",
        "LR_ovo_u_d = LogisticRegression()\n",
        "LR_ovo_u_d.fit(x_up_down_train,y_up_down_train)\n",
        "print(LR_ovo_u_d.score(x_up_down_test,y_up_down_test))\n",
        "\n",
        "LR_ovo_u_r = LogisticRegression()\n",
        "LR_ovo_u_r .fit(x_right_up_train,y_right_up_train)\n",
        "print(LR_ovo_u_r .score(x_right_up_test,y_right_up_test))\n",
        "\n",
        "LR_ovo_u_l = LogisticRegression()\n",
        "LR_ovo_u_l.fit(x_left_up_train,y_left_up_train)\n",
        "print(LR_ovo_u_l.score(x_left_up_test,y_left_up_test))\n",
        "\n",
        "LR_ovo_d_r = LogisticRegression()\n",
        "LR_ovo_d_r.fit(x_right_down_train,y_right_down_train)\n",
        "print(LR_ovo_d_r.score(x_right_down_test,y_right_down_test))\n",
        "\n",
        "LR_ovo_d_l = LogisticRegression()\n",
        "LR_ovo_d_l.fit(x_left_down_train,y_left_down_train)\n",
        "print(LR_ovo_d_l.score(x_left_down_test,y_left_down_test))\n",
        "\n",
        "LR_ovo_r_l = LogisticRegression()\n",
        "LR_ovo_r_l.fit(x_left_right_train,y_left_right_train)\n",
        "print(LR_ovo_r_l.score(x_left_right_test,y_left_right_test))\n",
        "\n",
        "LR_ovm_u = LogisticRegression()\n",
        "LR_ovm_u.fit(x_up_train,y_up_train)\n",
        "print(LR_ovm_u.score(x_up_test,y_up_test))\n",
        "\n",
        "LR_ovm_r = LogisticRegression()\n",
        "LR_ovm_r.fit(x_right_train,y_right_train)\n",
        "print(LR_ovm_r.score(x_right_test,y_right_test))\n",
        "\n",
        "LR_ovm_d = LogisticRegression()\n",
        "LR_ovm_d.fit(x_down_train,y_down_train)\n",
        "print(LR_ovm_d.score(x_down_test,y_down_test))\n",
        "\n",
        "LR_ovm_l = LogisticRegression()\n",
        "LR_ovm_l.fit(x_left_train,y_left_train)\n",
        "print(LR_ovm_l.score(x_left_test,y_left_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIK_MOLVsV6-"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "#initialising neural network\n",
        "model = models.Sequential()\n",
        "#specifying topology of neural network\n",
        "model.add(layers.Conv2D(22, (11,1), (1,1),input_shape=(375,22,1),padding=\"valid\"))\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "model.add(layers.SpatialDropout2D(.5))\n",
        "model.add(layers.Conv2D(44, (1,9), (1,1),padding=\"valid\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "model.add(layers.MaxPooling2D((2, 1),(2,1),padding=\"valid\"))\n",
        "model.add(layers.Conv2D(88, (11,1), (1,1),padding=\"valid\"))\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "model.add(layers.SpatialDropout2D(.5))\n",
        "model.add(layers.MaxPooling2D((2, 1),(2,1),padding=\"valid\"))\n",
        "model.add(layers.Conv2D(88, (11,1), (1,1),padding=\"valid\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "model.add(layers.SpatialDropout2D(.5))\n",
        "model.add(layers.MaxPooling2D((2, 1),(2,1),padding=\"valid\"))\n",
        "model.add(layers.Conv2D(176, (2,1), (1,1),padding=\"valid\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "model.add(layers.MaxPooling2D((2, 1),(2,1),padding=\"valid\"))\n",
        "model.add(layers.Conv2D(352, (2,1), (1,1),padding=\"valid\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "model.add(layers.MaxPooling2D((2, 1),(2,1),padding=\"valid\"))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(4))\n",
        "model.add(layers.Softmax())\n",
        "#specifying training mechanism\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "#training neural network\n",
        "model.fit(np.array(x_train_nn_space),np.array(y_train_nn_space),epochs=500,validation_data=(np.array(x_test_nn_space), np.array(y_test_nn_space)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-YEaVB7se-j"
      },
      "source": [
        "LDA_binary_outputs=[]\n",
        "LDA_ovo_outputs=[]\n",
        "LDA_ovm_outputs=[]\n",
        "LDA_multiclass_outputs=[]\n",
        "SVM_binary_outputs=[]\n",
        "SVM_ovo_outputs=[]\n",
        "SVM_ovm_outputs=[]\n",
        "SVM_multiclass_outputs=[]\n",
        "LR_binary_outputs=[]\n",
        "LR_ovo_outputs=[]\n",
        "LR_ovm_outputs=[]\n",
        "LR_multiclass_outputs=[]"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5OkfOHq4Acw"
      },
      "source": [
        "for i in x_all_test:\n",
        "  current=feature_extraction(i,csp_multiclass)\n",
        "  LDA_multiclass_outputs.append(multiclass_LDA.predict([current]))\n",
        "  SVM_multiclass_outputs.append(multiclass_SVM.predict([current]))\n",
        "  LR_multiclass_outputs.append(multiclass_LR.predict([current]))\n",
        "print(accuracy(LDA_multiclass_outputs,y_all_test))\n",
        "print(accuracy(SVM_multiclass_outputs,y_all_test))\n",
        "print(accuracy(LR_multiclass_outputs,y_all_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPeilqxwyjO5"
      },
      "source": [
        "for i in x_all_test:\n",
        "  current_1=feature_extraction(i,csp_filters_up_right)\n",
        "  current_2=feature_extraction(i,csp_filters_up_down)\n",
        "  current_3=feature_extraction(i,csp_filters_up_left)\n",
        "  current_4=feature_extraction(i,csp_filters_down_right)\n",
        "  current_5=feature_extraction(i,csp_filters_right_left)\n",
        "  current_6=feature_extraction(i,csp_filters_down_left)\n",
        "  output_1=LDA_ovo_u_r.predict([current_1])\n",
        "  output_2=LDA_ovo_u_d.predict([current_2])\n",
        "  output_3=LDA_ovo_u_l.predict([current_3])\n",
        "  output_4=LDA_ovo_d_r.predict([current_4])\n",
        "  output_5=LDA_ovo_r_l.predict([current_5])\n",
        "  output_6=LDA_ovo_d_l.predict([current_6])\n",
        "  LDA_ovo_outputs.append(one_v_one([output_1,output_2,output_3,output_4,output_5,output_6]))\n",
        "  output_1=SVM_ovo_u_r.predict([current_1])\n",
        "  output_2=SVM_ovo_u_d.predict([current_2])\n",
        "  output_3=SVM_ovo_u_l.predict([current_3])\n",
        "  output_4=SVM_ovo_d_r.predict([current_4])\n",
        "  output_5=SVM_ovo_r_l.predict([current_5])\n",
        "  output_6=SVM_ovo_d_l.predict([current_6])\n",
        "  SVM_ovo_outputs.append(one_v_one([output_1,output_2,output_3,output_4,output_5,output_6]))\n",
        "  output_1=LR_ovo_u_r.predict([current_1])\n",
        "  output_2=LR_ovo_u_d.predict([current_2])\n",
        "  output_3=LR_ovo_u_l.predict([current_3])\n",
        "  output_4=LR_ovo_d_r.predict([current_4])\n",
        "  output_5=LR_ovo_r_l.predict([current_5])\n",
        "  output_6=LR_ovo_d_l.predict([current_6])\n",
        "  LR_ovo_outputs.append(one_v_one([output_1,output_2,output_3,output_4,output_5,output_6]))\n",
        "\n",
        "print(accuracy(LDA_ovo_outputs,y_all_test))\n",
        "print(accuracy(SVM_ovo_outputs,y_all_test))\n",
        "print(accuracy(LR_ovo_outputs,y_all_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C9X64we6rhh"
      },
      "source": [
        "for i in x_all_test:\n",
        "  current_1=feature_extraction(i,csp_filters_v_h)\n",
        "  current_2=feature_extraction(i,csp_filters_up_down)\n",
        "  current_3=feature_extraction(i,csp_filters_right_left)\n",
        "  output_1=LDA_binary_search_v_h.predict([current_1])\n",
        "  output_2=LDA_ovo_u_d.predict([current_2])\n",
        "  output_3=LDA_ovo_r_l.predict([current_3])\n",
        "  LDA_binary_outputs.append(binary_search([output_1,output_2,output_3]))\n",
        "  output_1=SVM_binary_search_v_h.predict([current_1])\n",
        "  output_2=SVM_ovo_u_d.predict([current_2])\n",
        "  output_3=SVM_ovo_r_l.predict([current_3])\n",
        "  SVM_binary_outputs.append(binary_search([output_1,output_2,output_3]))\n",
        "  output_1=LR_binary_search_v_h.predict([current_1])\n",
        "  output_2=LR_ovo_u_d.predict([current_2])\n",
        "  output_3=LR_ovo_r_l.predict([current_3])\n",
        "  LR_binary_outputs.append(binary_search([output_1,output_2,output_3]))\n",
        "print(accuracy(LDA_binary_outputs,y_all_test))\n",
        "print(accuracy(SVM_binary_outputs,y_all_test))\n",
        "print(accuracy(LR_binary_outputs,y_all_test))50.0%\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLyF3oMu68yC"
      },
      "source": [
        "for i in x_all_test:\n",
        "  current_1=feature_extraction(i,csp_filters_up)\n",
        "  current_2=feature_extraction(i,csp_filters_down)\n",
        "  current_3=feature_extraction(i,csp_filters_left)\n",
        "  current_4=feature_extraction(i,csp_filters_right)\n",
        "  output_1=(LDA_ovm_u.predict_proba([current]))[0][0]\n",
        "  output_2=(LDA_ovm_d.predict_proba([current]))[0][0]\n",
        "  output_3=(LDA_ovm_l.predict_proba([current]))[0][0]\n",
        "  output_4=(LDA_ovm_r.predict_proba([current]))[0][0]\n",
        "  LDA_ovm_outputs.append(one_v_many([output_1,output_4,output_2,output_3]))\n",
        "  output_1=(SVM_ovm_u.predict_proba([current]))[0][0]\n",
        "  output_2=(SVM_ovm_d.predict_proba([current]))[0][0]\n",
        "  output_3=(SVM_ovm_l.predict_proba([current]))[0][0]\n",
        "  output_4=(SVM_ovm_r.predict_proba([current]))[0][0]\n",
        "  SVM_ovm_outputs.append(one_v_many([output_1,output_4,output_2,output_3]))\n",
        "  output_1=(LR_ovm_u.predict_proba([current]))[0][0]\n",
        "  output_2=(LR_ovm_d.predict_proba([current]))[0][0]\n",
        "  output_3=(LR_ovm_l.predict_proba([current]))[0][0]\n",
        "  output_4=(LR_ovm_r.predict_proba([current]))[0][0]\n",
        "  LR_ovm_outputs.append(one_v_many([output_1,output_4,output_2,output_3]))\n",
        "print(accuracy(LDA_ovm_outputs,y_all_test))\n",
        "print(accuracy(SVM_ovm_outputs,y_all_test))\n",
        "print(accuracy(LR_ovm_outputs,y_all_test))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}