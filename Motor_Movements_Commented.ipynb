{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Motor Movements Commented.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoufianeNoubir/Soufiane-Noubir-EEG-EPQ-2020-2021/blob/main/Motor_Movements_Commented.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY7_GzHN5RNL"
      },
      "source": [
        "Installing library for MAT files.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc8Hhq4Lg_za",
        "outputId": "e4776c97-47c0-4222-cc06-31c66cace372"
      },
      "source": [
        "pip install mat4py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mat4py\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/56/41b3ffd7b5f3eb3056979e0dc37de184c6e5dacd1a353e5de42323e7d138/mat4py-0.5.0-py2.py3-none-any.whl\n",
            "Installing collected packages: mat4py\n",
            "Successfully installed mat4py-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15G11kAs5XEA"
      },
      "source": [
        "Mounting Google drive to colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDCbs8bN0Fq_",
        "outputId": "d42fbc98-b801-417a-ce45-a4d7bfde489e"
      },
      "source": [
        "#Mounting Google drive to colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoAgNloYFRqF"
      },
      "source": [
        "Loading in the relevant files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uCj7HOTgPpF",
        "outputId": "00b8bf64-b52f-4511-c2ae-b9cf0fc01dd8"
      },
      "source": [
        "cd /content/drive/My Drive/EPQ datasets/5 finger data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/EPQ datasets/5 finger data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aGZjeuZgP3p",
        "outputId": "bb23002c-7612-4440-ffbe-2b5aeb0ea6cd"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/EPQ datasets/5 finger data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ06els0FeDC"
      },
      "source": [
        "Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UBjCeCuruBu"
      },
      "source": [
        "\n",
        "#Importing the libraries\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy.linalg import eigh\n",
        "import math\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trPgEE1OFxqP"
      },
      "source": [
        "Loading the dataset from the files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeJ_bn6igdLg"
      },
      "source": [
        "#Loading the dataset from the files\n",
        "#Due to time limitations described in the report I will only be using one participant whose data is in the file \"5F-SubjectI-160723-5St-SGLHand-HFREQ.mat\"\n",
        "#This participant used an EEG with a sample rate of 1000.\n",
        "from mat4py import loadmat\n",
        "data = loadmat('5F-SubjectI-160723-5St-SGLHand-HFREQ.mat')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NoZKvzQUcel"
      },
      "source": [
        "This section of code was finding the positions of relevant data  in the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir3X-rG6BFOd"
      },
      "source": [
        "\n",
        "#initialising variables\n",
        "und=[[0,0,0]]\n",
        "s=0\n",
        "z=0\n",
        "#The total period of time that this participant had the EEG on was just under an hour. \n",
        "#Since the sample rate of this dataset is 1000Hz the number of values measured by each channel is just under 3600000.\n",
        "#Exactly it is 3591000.\n",
        "#Each value has associated to it a marker.\n",
        "#A marker is a number that indicates what the participant was doing when that reading was taken.\n",
        "# data[\"o\"][\"marker\"] is a list of the markers for all EEG values. \n",
        "#I am only interested in the markers 1,2,3,4,5 as those indicate when the participant was flexing one of his 5 fingers.\n",
        "#The variable und is just a list that contains information about when there is a change of state i.e. when the participant stops doing a certain action.\n",
        "#Each element of the list und is a list itself.\n",
        "#The first element of these sub-lists give the activity that the participant has just stopped doing.\n",
        "#The second tells us how many values were measured during this uniterrupted period of time when the participant was just doing this activity.\n",
        "#Most importantly the final element tells us at what position in the EEG data this period stopped. \n",
        "#Since later on the data will be downsampled to 100Hz this position is scaled down by a factor of 10.\n",
        "for i in range(1,len(data[\"o\"][\"marker\"])):\n",
        "  if data[\"o\"][\"marker\"][i][0]!=data[\"o\"][\"marker\"][i-1][0]:\n",
        "    und.append([data[\"o\"][\"marker\"][i-1][0],i-z,i-1])\n",
        "    z=i\n",
        "for i in und:\n",
        "  s+=i[1]\n",
        "for i in und:\n",
        "  i[2]=(i[2]//10)+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jttTAAHXoMt_"
      },
      "source": [
        "This code transposes the data so that its dimensions are easier to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz9pY--wJv4S"
      },
      "source": [
        "# data[\"o\"][\"data\"] is a 3591000 by 22 marix containing all the measurementes of each electrode throughout the experiment.\n",
        "#Here the raw data matrix was transposed so that it would be a 22 by 3591000 matrix.\n",
        "#To transpose a matrix in the form of a nested list you have to turn into a numpy array then transpose it and then turn it back into a list.\n",
        "import numpy as np\n",
        "raw_data=data[\"o\"][\"data\"]\n",
        "raw_data=np.array(raw_data)\n",
        "raw_data=np.transpose(raw_data)\n",
        "raw_data=list(raw_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QiiNDbIkSkv"
      },
      "source": [
        "Function that removes unwanted electrodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxLrJoa3NFG8"
      },
      "source": [
        "##removes unwanted electrodes\n",
        "#This function was not needed on this dataset as all 22 electrodes need to be used.\n",
        "def remove_electrodes(data,numbers):\n",
        "  output=[]\n",
        "  for i in range(len(data)):\n",
        "    if i in numbers:\n",
        "      \n",
        "      output.append(data[i])\n",
        "  \n",
        "  return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpRSp6LQkV9U"
      },
      "source": [
        "Function that downsamples the data from 1000Hz to 100Hz by only using every tenth value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLz-70QcNI3-"
      },
      "source": [
        "#function that takes in the data and its sample rate which is either 200Hz or 1000Hz(the plan was to use mutliple files with different sample rates) \n",
        "#at 200Hz the data is downsampled by a factor of 2\n",
        "#at 1000Hz the data is downsampled by a factor of 10\n",
        "def downsample(data,x):\n",
        "  output=[]\n",
        "  current=[]\n",
        "  for i in data:\n",
        "    for j in range(len(i)):\n",
        "      if j%(x/100)==0:\n",
        "        current.append(i[j])\n",
        "    output.append(current)\n",
        "    current=[]\n",
        "  return output\n",
        "# the recordings in this file was done at 1000Hz\n",
        "raw_data_downsample=downsample(raw_data,1000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j024jS_NdmD"
      },
      "source": [
        "#1=thumb\n",
        "finger_1=[]\n",
        "#2=ring\n",
        "finger_2=[]\n",
        "#3=middle\n",
        "finger_3=[]\n",
        "#4=index\n",
        "finger_4=[]\n",
        "#5=little\n",
        "finger_5=[]\n",
        "current=[]\n",
        "for i in range(len(und[1:])):\n",
        "  if und[i][0]==1:\n",
        "    for j in raw_data_downsample:\n",
        "      current.append(j[und[i-1][2]:und[i][2]])\n",
        "    finger_1.append(current)\n",
        "    current=[]\n",
        "  if und[i][0]==2:\n",
        "    for j in raw_data_downsample:\n",
        "      current.append(j[und[i-1][2]:und[i][2]])\n",
        "    finger_2.append(current)\n",
        "    current=[]\n",
        "  if und[i][0]==3:\n",
        "    for j in raw_data_downsample:\n",
        "      current.append(j[und[i-1][2]:und[i][2]])\n",
        "    finger_3.append(current)\n",
        "    current=[]\n",
        "  if und[i][0]==4:\n",
        "    for j in raw_data_downsample:\n",
        "      current.append(j[und[i-1][2]:und[i][2]])\n",
        "    finger_4.append(current)\n",
        "    current=[]\n",
        "  if und[i][0]==5:\n",
        "    for j in raw_data_downsample:\n",
        "      current.append(j[und[i-1][2]:und[i][2]])\n",
        "    finger_5.append(current)\n",
        "    current=[]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoBIdiCyxU0m"
      },
      "source": [
        "Splitting up the data into 4 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zEnsT3jjJIT"
      },
      "source": [
        "#flexing the thumb corresponds to moving the cursor up\n",
        "#flexing the ring finger corresponds to moving the cursor right\n",
        "#flexing the middle finger corresponds to moving the cursor down\n",
        "#flexing the index finger corresponds to moving the cursor left\n",
        "#as discussed in the report only 4 classes will be used and the little finger will be ignored.\n",
        "#as discussed in the report there was variation in the length of each trial and they all had to be cut down to 127 measurements or 1.27 seconds which was the length of the shortest trial.\n",
        "\n",
        "#up_data contains the downsampled and cut down data for moving the cursor up.\n",
        "up_data=[]\n",
        "#down_data contains the downsampled and cut down data for moving the cursor down.\n",
        "down_data=[]\n",
        "#left_data contains the downsampled and cut down data for moving the cursor left.\n",
        "left_data=[]\n",
        "#right_data contains the downsampled and cut down data for moving the cursor right.\n",
        "right_data=[]\n",
        "for i in finger_1:\n",
        "  current=[]\n",
        "  for j in i:\n",
        "    current.append(j[:127])\n",
        "  up_data.append(current)\n",
        "for i in finger_2:\n",
        "  current=[]\n",
        "  for j in i:\n",
        "    current.append(j[:127])\n",
        "  right_data.append(current)\n",
        "for i in finger_3:\n",
        "  current=[]\n",
        "  for j in i:\n",
        "    current.append(j[:127])\n",
        "  down_data.append(current)\n",
        "for i in finger_4:\n",
        "  current=[]\n",
        "  for j in i:\n",
        "    current.append(j[:127])\n",
        "  left_data.append(current)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32xdbmGgxayo"
      },
      "source": [
        "Splitting the data into training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UyzNBgYkMJP"
      },
      "source": [
        "#splitting of dataset into training and testing data\n",
        "\n",
        "#left_data_evaluation contains the test data for moving the cursor left and so on\n",
        "#datapoints from left_data is removed and placed into left_data_evaluation with a probability of 1/4. \n",
        "#Therefore, overall the models will be trained on 3/4 the size of initial dataset and tested on the other quarter.\n",
        "#same is done for all 4 classes\n",
        "left_data_evaluation=[]\n",
        "right_data_evaluation=[]\n",
        "down_data_evaluation=[]\n",
        "up_data_evaluation=[]\n",
        "import random\n",
        "for i in up_data:\n",
        "  x=random.randint(1,4)\n",
        "  if x==4:\n",
        "    up_data.remove(i)\n",
        "    up_data_evaluation.append(i)\n",
        "for i in right_data:\n",
        "  x=random.randint(1,4)\n",
        "  if x==4:\n",
        "    right_data.remove(i)\n",
        "    right_data_evaluation.append(i)\n",
        "for i in down_data:\n",
        "  x=random.randint(1,4)\n",
        "  if x==4:\n",
        "    down_data.remove(i)\n",
        "    down_data_evaluation.append(i)\n",
        "for i in left_data:\n",
        "  x=random.randint(1,4)\n",
        "  if x==4:\n",
        "    left_data.remove(i)\n",
        "    left_data_evaluation.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJKmk-PXe0Fk"
      },
      "source": [
        "The rest of the code is almost exactly identical to the code in the Spatial Imagery file as the analysis done is precisely the same. Therefore I will not comment this code as the comments are available in the other file. The only difference that is important to note is that the data here has dimension 22 by 127 rather than 22 by 375 because of the different sample rate and length of trial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXsyxiuMqcRh"
      },
      "source": [
        "space_data=[]\n",
        "space_data.append(up_data)\n",
        "space_data.append(right_data)\n",
        "space_data.append(down_data)\n",
        "space_data.append(left_data)\n",
        "space_data_evaluation=[]\n",
        "space_data_evaluation.append(up_data_evaluation)\n",
        "space_data_evaluation.append(right_data_evaluation)\n",
        "space_data_evaluation.append(down_data_evaluation)\n",
        "space_data_evaluation.append(left_data_evaluation)\n",
        "x_all_test=[]\n",
        "y_all_test=[]\n",
        "x_all_test+=up_data_evaluation\n",
        "x_all_test+=right_data_evaluation\n",
        "x_all_test+=down_data_evaluation\n",
        "x_all_test+=left_data_evaluation\n",
        "for i in up_data_evaluation:\n",
        "  y_all_test.append(1)\n",
        "for i in right_data_evaluation:\n",
        "  y_all_test.append(2)\n",
        "for i in down_data_evaluation:\n",
        "  y_all_test.append(3)\n",
        "for i in left_data_evaluation:\n",
        "  y_all_test.append(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmXfw81EqevN"
      },
      "source": [
        "#implementation of standard CSP approach\n",
        "def csp(data_1,data_2):\n",
        "  mean_cov_1=np.cov(np.array(data_1[0]))\n",
        "  \n",
        "  mean_cov_2=np.cov(np.array(data_2[0]))\n",
        " \n",
        "  for i in data_1[1:]:\n",
        "    mean_cov_1+=np.cov(np.array(i))\n",
        "    \n",
        "  for i in data_2[1:]:\n",
        "    mean_cov_2+=np.cov(np.array(i))\n",
        "    \n",
        "  mean_cov_1=mean_cov_1/len(data_1)\n",
        "  mean_cov_2=mean_cov_2/len(data_2)\n",
        "\n",
        "  \n",
        "  w_1, v_1 = np.linalg.eig(np.dot(np.linalg.inv(mean_cov_2+mean_cov_1),mean_cov_1))\n",
        "  sort=np.argsort(w_1)\n",
        "  \n",
        "  \n",
        "  return ([v_1[sort[0]],v_1[sort[1]],v_1[sort[2]],v_1[sort[-3]],v_1[sort[-2]],v_1[sort[-1]]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Qowqb0TqfYi"
      },
      "source": [
        "#implementation of multiclass CSP\n",
        "def multiclass_csp(data):\n",
        "  total_elements=0\n",
        "  for i in data:\n",
        "    total_elements+=len(i)\n",
        "  average=[]\n",
        "  for i in data:\n",
        "    total=np.array(i[0])\n",
        "    \n",
        "    for j in i[1:]:\n",
        "      \n",
        "      total+=np.array(j)\n",
        "    total=total/len(i)\n",
        "    \n",
        "      \n",
        "    average.append(total)\n",
        "  total_average=len(data[0])*average[0]\n",
        "  for i in range(len(data[1:])):\n",
        "    total_average+=len(data[i])*average[i]\n",
        "  total_average=total_average/total_elements\n",
        "  within_class=np.dot(data[0][0]-average[0],(data[0][0]-average[0]).transpose())\n",
        "  for i in range(len(data)):\n",
        "    \n",
        "    for j in data[i]:\n",
        "      within_class+=np.dot(j-average[i],(j-average[i]).transpose())\n",
        "  within_class-=np.dot(data[0][0]-average[0],(data[0][0]-average[0]).transpose())\n",
        "  within_class=within_class/total_elements\n",
        "  between_class=len(data[0])*(np.dot(average[0]-total_average,(average[0]-total_average).transpose()))\n",
        "  for i in range(len(average[1:])):\n",
        "    between_class+=len(data[i])*(np.dot(average[i]-total_average,(average[i]-total_average).transpose()))\n",
        "  between_class=between_class/len(data)\n",
        "  w_1, v_1 = np.linalg.eig(np.dot(np.linalg.inv(between_class+within_class),between_class))\n",
        "  sort=np.argsort(w_1)\n",
        "  return ([v_1[sort[0]],v_1[sort[1]],v_1[sort[2]],v_1[sort[3]],v_1[sort[4]],v_1[sort[5]]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YNoph5gqk6V"
      },
      "source": [
        "# implementation of feature extraction using filters generate from CSP\n",
        "def feature_extraction(up_data,filters):\n",
        "  features=[]\n",
        "  for i in filters:\n",
        "    features.append(math.log(np.var(np.dot(i.transpose(),up_data))))\n",
        "    \n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfaEw2-sqmvV"
      },
      "source": [
        "csp_filters_left=csp(left_data,right_data+up_data+down_data)\n",
        "csp_filters_right=csp(right_data,left_data+up_data+down_data)\n",
        "csp_filters_down=csp(down_data,right_data+up_data+left_data)\n",
        "csp_filters_up=csp(up_data,right_data+left_data+down_data)\n",
        "csp_filters_up_down=csp(up_data,down_data)\n",
        "csp_filters_down_right=csp(right_data,down_data)\n",
        "csp_filters_up_right=csp(up_data,right_data)\n",
        "csp_filters_down_left=csp(left_data,down_data)\n",
        "csp_filters_right_left=csp(right_data,left_data)\n",
        "csp_filters_up_left=csp(up_data,left_data)\n",
        "csp_filters_v_h=csp(up_data+down_data,right_data+left_data)\n",
        "csp_multiclass=multiclass_csp(space_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzF7NWA9qnTD"
      },
      "source": [
        "x_vertical_horizontal_train=[]\n",
        "y_vertical_horizontal_train=[]\n",
        "x_vertical_horizontal_test=[]\n",
        "y_vertical_horizontal_test=[]\n",
        "x_up_down_train=[]\n",
        "y_up_down_train=[]\n",
        "x_up_down_test=[]\n",
        "y_up_down_test=[]\n",
        "x_left_right_train=[]\n",
        "y_left_right_train=[]\n",
        "x_left_right_test=[]\n",
        "y_left_right_test=[]\n",
        "x_left_train=[]\n",
        "y_left_train=[]\n",
        "x_left_test=[]\n",
        "y_left_test=[]\n",
        "x_right_train=[]\n",
        "y_right_train=[]\n",
        "x_right_test=[]\n",
        "y_right_test=[]\n",
        "x_up_train=[]\n",
        "y_up_train=[]\n",
        "x_up_test=[]\n",
        "y_up_test=[]\n",
        "x_down_train=[]\n",
        "y_down_train=[]\n",
        "x_down_test=[]\n",
        "y_down_test=[]\n",
        "x_right_up_train=[]\n",
        "y_right_up_train=[]\n",
        "x_right_up_test=[]\n",
        "y_right_up_test=[]\n",
        "x_right_down_train=[]\n",
        "y_right_down_train=[]\n",
        "x_right_down_test=[]\n",
        "y_right_down_test=[]\n",
        "x_left_up_train=[]\n",
        "y_left_up_train=[]\n",
        "x_left_up_test=[]\n",
        "y_left_up_test=[]\n",
        "x_left_down_train=[]\n",
        "y_left_down_train=[]\n",
        "x_left_down_test=[]\n",
        "y_left_down_test=[]\n",
        "x_csp_space_train=[]\n",
        "y_csp_space_train=[]\n",
        "x_csp_space_test=[]\n",
        "y_csp_space_test=[]\n",
        "x_train_nn_space=[]\n",
        "y_train_nn_space=[]\n",
        "x_test_nn_space=[]\n",
        "y_test_nn_space=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVStOhuzqutW"
      },
      "source": [
        "for i in up_data:\n",
        "  x_vertical_horizontal_train.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in down_data:\n",
        "  x_vertical_horizontal_train.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in right_data:\n",
        "  x_vertical_horizontal_train.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in left_data:\n",
        "  x_vertical_horizontal_train.append(feature_extraction(i,csp_filters_v_h))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_vertical_horizontal_test.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in down_data_evaluation:\n",
        "  x_vertical_horizontal_test.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in right_data_evaluation:\n",
        "  x_vertical_horizontal_test.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in left_data_evaluation:\n",
        "  x_vertical_horizontal_test.append(feature_extraction(i,csp_filters_v_h))\n",
        "\n",
        "for i in up_data:\n",
        "  x_up_down_train.append(feature_extraction(i,csp_filters_up_down))\n",
        "for i in down_data:\n",
        "  x_up_down_train.append(feature_extraction(i,csp_filters_up_down))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_up_down_test.append(feature_extraction(i,csp_filters_up_down))\n",
        "for i in down_data_evaluation:\n",
        "  x_up_down_test.append(feature_extraction(i,csp_filters_up_down))\n",
        "\n",
        "for i in right_data:\n",
        "  x_left_right_train.append(feature_extraction(i,csp_filters_right_left))\n",
        "for i in left_data:\n",
        "  x_left_right_train.append(feature_extraction(i,csp_filters_right_left))\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  x_left_right_test.append(feature_extraction(i,csp_filters_right_left))\n",
        "for i in left_data_evaluation:\n",
        "  x_left_right_test.append(feature_extraction(i,csp_filters_right_left))\n",
        "\n",
        "for i in right_data:\n",
        "  x_right_up_train.append(feature_extraction(i,csp_filters_up_right))\n",
        "for i in up_data:\n",
        "  x_right_up_train.append(feature_extraction(i,csp_filters_up_right))\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  x_right_up_test.append(feature_extraction(i,csp_filters_up_right))\n",
        "for i in up_data_evaluation:\n",
        "  x_right_up_test.append(feature_extraction(i,csp_filters_up_right))\n",
        "\n",
        "for i in right_data:\n",
        "  x_right_down_train.append(feature_extraction(i,csp_filters_down_right))\n",
        "for i in down_data:\n",
        "  x_right_down_train.append(feature_extraction(i,csp_filters_down_right))\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  x_right_down_test.append(feature_extraction(i,csp_filters_down_right))\n",
        "for i in down_data_evaluation:\n",
        "  x_right_down_test.append(feature_extraction(i,csp_filters_down_right))\n",
        "\n",
        "for i in left_data:\n",
        "  x_left_up_train.append(feature_extraction(i,csp_filters_up_left))\n",
        "for i in up_data:\n",
        "  x_left_up_train.append(feature_extraction(i,csp_filters_up_left))\n",
        "\n",
        "for i in left_data_evaluation:\n",
        "  x_left_up_test.append(feature_extraction(i,csp_filters_up_left))\n",
        "for i in up_data_evaluation:\n",
        "  x_left_up_test.append(feature_extraction(i,csp_filters_up_left))\n",
        "\n",
        "\n",
        "for i in left_data:\n",
        "  x_left_down_train.append(feature_extraction(i,csp_filters_down_left))\n",
        "for i in down_data:\n",
        "  x_left_down_train.append(feature_extraction(i,csp_filters_down_left))\n",
        "\n",
        "for i in left_data_evaluation:\n",
        "  x_left_down_test.append(feature_extraction(i,csp_filters_down_left))\n",
        "for i in down_data_evaluation:\n",
        "  x_left_down_test.append(feature_extraction(i,csp_filters_down_left))\n",
        "\n",
        "\n",
        "for i in up_data:\n",
        "  x_left_train.append(feature_extraction(i,csp_filters_left))\n",
        "for i in down_data:\n",
        "  x_left_train.append(feature_extraction(i,csp_filters_left))\n",
        "for i in right_data:\n",
        "  x_left_train.append(feature_extraction(i,csp_filters_left))\n",
        "for i in left_data:\n",
        "  x_left_train.append(feature_extraction(i,csp_filters_left))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_left_test.append(feature_extraction(i,csp_filters_left))\n",
        "for i in down_data_evaluation:\n",
        "  x_left_test.append(feature_extraction(i,csp_filters_left))\n",
        "for i in right_data_evaluation:\n",
        "  x_left_test.append(feature_extraction(i,csp_filters_left))\n",
        "for i in left_data_evaluation:\n",
        "  x_left_test.append(feature_extraction(i,csp_filters_left))\n",
        "\n",
        "for i in up_data:\n",
        "  x_right_train.append(feature_extraction(i,csp_filters_right))\n",
        "for i in down_data:\n",
        "  x_right_train.append(feature_extraction(i,csp_filters_right))\n",
        "for i in right_data:\n",
        "  x_right_train.append(feature_extraction(i,csp_filters_right))\n",
        "for i in left_data:\n",
        "  x_right_train.append(feature_extraction(i,csp_filters_right))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_right_test.append(feature_extraction(i,csp_filters_right))\n",
        "for i in down_data_evaluation:\n",
        "  x_right_test.append(feature_extraction(i,csp_filters_right))\n",
        "for i in right_data_evaluation:\n",
        "  x_right_test.append(feature_extraction(i,csp_filters_right))\n",
        "for i in left_data_evaluation:\n",
        "  x_right_test.append(feature_extraction(i,csp_filters_right))\n",
        "\n",
        "for i in up_data:\n",
        "  x_up_train.append(feature_extraction(i,csp_filters_up))\n",
        "for i in down_data:\n",
        "  x_up_train.append(feature_extraction(i,csp_filters_up))\n",
        "for i in right_data:\n",
        "  x_up_train.append(feature_extraction(i,csp_filters_up))\n",
        "for i in left_data:\n",
        "  x_up_train.append(feature_extraction(i,csp_filters_up))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_up_test.append(feature_extraction(i,csp_filters_up))\n",
        "for i in down_data_evaluation:\n",
        "  x_up_test.append(feature_extraction(i,csp_filters_up))\n",
        "for i in right_data_evaluation:\n",
        "  x_up_test.append(feature_extraction(i,csp_filters_up))\n",
        "for i in left_data_evaluation:\n",
        "  x_up_test.append(feature_extraction(i,csp_filters_up))\n",
        "\n",
        "for i in up_data:\n",
        "  x_down_train.append(feature_extraction(i,csp_filters_down))\n",
        "for i in down_data:\n",
        "  x_down_train.append(feature_extraction(i,csp_filters_down))\n",
        "for i in right_data:\n",
        "  x_down_train.append(feature_extraction(i,csp_filters_down))\n",
        "for i in left_data:\n",
        "  x_down_train.append(feature_extraction(i,csp_filters_down))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_down_test.append(feature_extraction(i,csp_filters_down))\n",
        "for i in down_data_evaluation:\n",
        "  x_down_test.append(feature_extraction(i,csp_filters_down))\n",
        "for i in right_data_evaluation:\n",
        "  x_down_test.append(feature_extraction(i,csp_filters_down))\n",
        "for i in left_data_evaluation:\n",
        "  x_down_test.append(feature_extraction(i,csp_filters_down))\n",
        "\n",
        "for i in up_data:\n",
        "  x_csp_space_train.append(feature_extraction(i,csp_multiclass))\n",
        "for i in down_data:\n",
        "  x_csp_space_train.append(feature_extraction(i,csp_multiclass))\n",
        "for i in right_data:\n",
        "  x_csp_space_train.append(feature_extraction(i,csp_multiclass))\n",
        "for i in left_data:\n",
        "  x_csp_space_train.append(feature_extraction(i,csp_multiclass))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_csp_space_test.append(feature_extraction(i,csp_multiclass))\n",
        "for i in down_data_evaluation:\n",
        "  x_csp_space_test.append(feature_extraction(i,csp_multiclass))\n",
        "for i in right_data_evaluation:\n",
        "  x_csp_space_test.append(feature_extraction(i,csp_multiclass))\n",
        "for i in left_data_evaluation:\n",
        "  x_csp_space_test.append(feature_extraction(i,csp_multiclass))\n",
        "\n",
        "\n",
        "for i in space_data:\n",
        "  \n",
        "  for j in i:\n",
        "    intermediate=np.transpose(j)\n",
        "    intermediate_3=[]\n",
        "    for k in intermediate:\n",
        "      intermediate_2=[]\n",
        "      for l in k:\n",
        "        intermediate_2.append(np.array([l]))\n",
        "      intermediate_3.append(np.array(intermediate_2))\n",
        "    x_train_nn_space.append(np.array(intermediate_3))\n",
        "for i in up_data:\n",
        "  y_train_nn_space.append(np.array([1,0,0,0]))\n",
        "for i in right_data:\n",
        "  y_train_nn_space.append(np.array([0,1,0,0]))\n",
        "for i in down_data:\n",
        "  y_train_nn_space.append(np.array([0,0,1,0]))\n",
        "for i in left_data:\n",
        "  y_train_nn_space.append(np.array([0,0,0,1]))\n",
        "\n",
        "for i in space_data_evaluation:\n",
        "  \n",
        "  for j in i:\n",
        "    intermediate=np.transpose(j)\n",
        "    intermediate_3=[]\n",
        "    for k in intermediate:\n",
        "      intermediate_2=[]\n",
        "      for l in k:\n",
        "        intermediate_2.append(np.array([l]))\n",
        "      intermediate_3.append(np.array(intermediate_2))   \n",
        "    \n",
        "    x_test_nn_space.append(np.array(intermediate_3))\n",
        "    \n",
        "for i in up_data_evaluation:\n",
        "  y_test_nn_space.append(np.array([1,0,0,0]))\n",
        "for i in right_data_evaluation:\n",
        "  y_test_nn_space.append(np.array([0,1,0,0]))\n",
        "for i in down_data_evaluation:\n",
        "  y_test_nn_space.append(np.array([0,0,1,0]))\n",
        "for i in left_data_evaluation:\n",
        "  y_test_nn_space.append(np.array([0,0,0,1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9oXDx1sqzKI"
      },
      "source": [
        "for i in up_data:\n",
        " y_vertical_horizontal_train.append(1)\n",
        "for i in down_data:\n",
        "  y_vertical_horizontal_train.append(1)\n",
        "for i in right_data:\n",
        "  y_vertical_horizontal_train.append(2)\n",
        "for i in left_data:\n",
        "  y_vertical_horizontal_train.append(2)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_vertical_horizontal_test.append(1)\n",
        "for i in down_data_evaluation:\n",
        "  y_vertical_horizontal_test.append(1)\n",
        "for i in right_data_evaluation:\n",
        "  y_vertical_horizontal_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_vertical_horizontal_test.append(2)\n",
        "\n",
        "for i in up_data:\n",
        "  y_up_down_train.append(1)\n",
        "for i in down_data:\n",
        "  y_up_down_train.append(3)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_up_down_test.append(1)\n",
        "for i in down_data_evaluation:\n",
        "  y_up_down_test.append(3)\n",
        "\n",
        "for i in right_data:\n",
        "  y_left_right_train.append(2)\n",
        "for i in left_data:\n",
        "  y_left_right_train.append(4)\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  y_left_right_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_left_right_test.append(4)\n",
        "\n",
        "for i in right_data:\n",
        "  y_right_up_train.append(2)\n",
        "for i in up_data:\n",
        "  y_right_up_train.append(1)\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  y_right_up_test.append(2)\n",
        "for i in up_data_evaluation:\n",
        "  y_right_up_test.append(1)\n",
        "\n",
        "for i in right_data:\n",
        "  y_right_down_train.append(2)\n",
        "for i in down_data:\n",
        "  y_right_down_train.append(3)\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  y_right_down_test.append(2)\n",
        "for i in down_data_evaluation:\n",
        "  y_right_down_test.append(3)\n",
        "\n",
        "for i in left_data:\n",
        "  y_left_up_train.append(4)\n",
        "for i in up_data:\n",
        "  y_left_up_train.append(1)\n",
        "\n",
        "for i in left_data_evaluation:\n",
        "  y_left_up_test.append(4)\n",
        "for i in up_data_evaluation:\n",
        "  y_left_up_test.append(1)\n",
        "\n",
        "for i in left_data:\n",
        "  y_left_down_train.append(4)\n",
        "for i in down_data:\n",
        "  y_left_down_train.append(3)\n",
        "\n",
        "for i in left_data_evaluation:\n",
        "  y_left_down_test.append(4)\n",
        "for i in down_data_evaluation:\n",
        "  y_left_down_test.append(3)\n",
        "\n",
        "for i in up_data:\n",
        "  y_left_train.append(2)\n",
        "for i in down_data:\n",
        "  y_left_train.append(2)\n",
        "for i in right_data:\n",
        "  y_left_train.append(2)\n",
        "for i in left_data:\n",
        "  y_left_train.append(1)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_left_test.append(2)\n",
        "for i in down_data_evaluation:\n",
        "  y_left_test.append(2)\n",
        "for i in right_data_evaluation:\n",
        "  y_left_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_left_test.append(1)\n",
        "\n",
        "for i in up_data:\n",
        "  y_right_train.append(2)\n",
        "for i in down_data:\n",
        "  y_right_train.append(2)\n",
        "for i in right_data:\n",
        "  y_right_train.append(1)\n",
        "for i in left_data:\n",
        "  y_right_train.append(2)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_right_test.append(2)\n",
        "for i in down_data_evaluation:\n",
        "  y_right_test.append(2)\n",
        "for i in right_data_evaluation:\n",
        "  y_right_test.append(1)\n",
        "for i in left_data_evaluation:\n",
        "  y_right_test.append(2)\n",
        "\n",
        "for i in up_data:\n",
        "  y_up_train.append(1)\n",
        "for i in down_data:\n",
        "  y_up_train.append(2)\n",
        "for i in right_data:\n",
        "  y_up_train.append(2)\n",
        "for i in left_data:\n",
        "  y_up_train.append(2)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_up_test.append(1)\n",
        "for i in down_data_evaluation:\n",
        "  y_up_test.append(2)\n",
        "for i in right_data_evaluation:\n",
        "  y_up_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_up_test.append(2)\n",
        "\n",
        "for i in up_data:\n",
        "  y_down_train.append(2)\n",
        "for i in down_data:\n",
        "  y_down_train.append(1)\n",
        "for i in right_data:\n",
        "  y_down_train.append(2)\n",
        "for i in left_data:\n",
        "  y_down_train.append(2)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_down_test.append(2)\n",
        "for i in down_data_evaluation:\n",
        "  y_down_test.append(1)\n",
        "for i in right_data_evaluation:\n",
        "  y_down_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_down_test.append(2)\n",
        "\n",
        "for i in up_data:\n",
        "  y_csp_space_train.append(1)\n",
        "for i in down_data:\n",
        "  y_csp_space_train.append(3)\n",
        "for i in right_data:\n",
        "  y_csp_space_train.append(2)\n",
        "for i in left_data:\n",
        "  y_csp_space_train.append(4)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_csp_space_test.append(1)\n",
        "for i in down_data_evaluation:\n",
        "  y_csp_space_test.append(3)\n",
        "for i in right_data_evaluation:\n",
        "  y_csp_space_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_csp_space_test.append(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDa5LU8_q2MK"
      },
      "source": [
        "#uses one_v_one classification to output the predicted class\n",
        "def one_v_one(outputs):\n",
        "  num_1=outputs.count(1)\n",
        "  num_2=outputs.count(2)\n",
        "  num_3=outputs.count(3)\n",
        "  num_4=outputs.count(4)\n",
        "  num=[num_1,num_2,num_3,num_4]\n",
        "  if num.count(max(num))==1:\n",
        "    return (num.index(max(num)))+1\n",
        "  else:\n",
        "    \n",
        "    winners=[]\n",
        "    for i in range(len(num)):\n",
        "      if num[i] ==max(num):\n",
        "        winners.append(i+1)\n",
        "    if len(winners)==3:\n",
        "      return (\"alert\")\n",
        "    else:\n",
        "      if min(winners)==1:\n",
        "        return outputs[sum(winners)-3]\n",
        "      else:\n",
        "        return outputs[sum(winners)-2]\n",
        "#uses one_v_many classification to output the predicted class\n",
        "def one_v_many(probs):\n",
        "  return (probs.index(max(probs))+1)\n",
        "#uses binary search to output the predicted class\n",
        "def binary_search(outputs):\n",
        "  if outputs[0]==1:\n",
        "    return outputs[1]\n",
        "  else:\n",
        "    return outputs[2]\n",
        "#returns accuracy\n",
        "def accuracy(outputs,correct):\n",
        "  right=0\n",
        "  for i in range(len(outputs)):\n",
        "    if outputs[i]==correct[i]:\n",
        "      right+=1\n",
        "  return right/(len(outputs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2WMexWgq237"
      },
      "source": [
        "multiclass_LDA = LinearDiscriminantAnalysis()\n",
        "multiclass_LDA.fit(x_csp_space_train,y_csp_space_train)\n",
        "print(multiclass_LDA.score(x_csp_space_test,y_csp_space_test))\n",
        "\n",
        "LDA_binary_search_v_h = LinearDiscriminantAnalysis()\n",
        "LDA_binary_search_v_h.fit(x_vertical_horizontal_train,y_vertical_horizontal_train)\n",
        "print(LDA_binary_search_v_h.score(x_vertical_horizontal_test,y_vertical_horizontal_test))\n",
        "\n",
        "LDA_ovo_u_d = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_u_d.fit(x_up_down_train,y_up_down_train)\n",
        "print(LDA_ovo_u_d.score(x_up_down_test,y_up_down_test))\n",
        "\n",
        "LDA_ovo_u_r = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_u_r .fit(x_right_up_train,y_right_up_train)\n",
        "print(LDA_ovo_u_r .score(x_right_up_test,y_right_up_test))\n",
        "\n",
        "LDA_ovo_u_l = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_u_l.fit(x_left_up_train,y_left_up_train)\n",
        "print(LDA_ovo_u_l.score(x_left_up_test,y_left_up_test))\n",
        "\n",
        "LDA_ovo_d_r = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_d_r.fit(x_right_down_train,y_right_down_train)\n",
        "print(LDA_ovo_d_r.score(x_right_down_test,y_right_down_test))\n",
        "\n",
        "LDA_ovo_d_l = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_d_l.fit(x_left_down_train,y_left_down_train)\n",
        "print(LDA_ovo_d_l.score(x_left_down_test,y_left_down_test))\n",
        "\n",
        "LDA_ovo_r_l = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_r_l.fit(x_left_right_train,y_left_right_train)\n",
        "print(LDA_ovo_r_l.score(x_left_right_test,y_left_right_test))\n",
        "\n",
        "LDA_ovm_u = LinearDiscriminantAnalysis()\n",
        "LDA_ovm_u.fit(x_up_train,y_up_train)\n",
        "print(LDA_ovm_u.score(x_up_test,y_up_test))\n",
        "\n",
        "LDA_ovm_r = LinearDiscriminantAnalysis()\n",
        "LDA_ovm_r.fit(x_right_train,y_right_train)\n",
        "print(LDA_ovm_r.score(x_right_test,y_right_test))\n",
        "\n",
        "LDA_ovm_d = LinearDiscriminantAnalysis()\n",
        "LDA_ovm_d.fit(x_down_train,y_down_train)\n",
        "print(LDA_ovm_d.score(x_down_test,y_down_test))\n",
        "\n",
        "LDA_ovm_l = LinearDiscriminantAnalysis()\n",
        "LDA_ovm_l.fit(x_left_train,y_left_train)\n",
        "print(LDA_ovm_l.score(x_left_test,y_left_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMjgezygq8tM"
      },
      "source": [
        "multiclass_SVM = svm.SVC(probability=True)\n",
        "multiclass_SVM.fit(x_csp_space_train,y_csp_space_train)\n",
        "print(multiclass_SVM.score(x_csp_space_test,y_csp_space_test))\n",
        "\n",
        "SVM_binary_search_v_h = svm.SVC(probability=True)\n",
        "SVM_binary_search_v_h.fit(x_vertical_horizontal_train,y_vertical_horizontal_train)\n",
        "print(SVM_binary_search_v_h.score(x_vertical_horizontal_test,y_vertical_horizontal_test))\n",
        "\n",
        "SVM_ovo_u_d = svm.SVC(probability=True)\n",
        "SVM_ovo_u_d.fit(x_up_down_train,y_up_down_train)\n",
        "print(SVM_ovo_u_d.score(x_up_down_test,y_up_down_test))\n",
        "\n",
        "SVM_ovo_u_r = svm.SVC(probability=True)\n",
        "SVM_ovo_u_r .fit(x_right_up_train,y_right_up_train)\n",
        "print(SVM_ovo_u_r .score(x_right_up_test,y_right_up_test))\n",
        "\n",
        "SVM_ovo_u_l = svm.SVC(probability=True)\n",
        "SVM_ovo_u_l.fit(x_left_up_train,y_left_up_train)\n",
        "print(SVM_ovo_u_l.score(x_left_up_test,y_left_up_test))\n",
        "\n",
        "SVM_ovo_d_r = svm.SVC(probability=True)\n",
        "SVM_ovo_d_r.fit(x_right_down_train,y_right_down_train)\n",
        "print(SVM_ovo_d_r.score(x_right_down_test,y_right_down_test))\n",
        "\n",
        "SVM_ovo_d_l = svm.SVC(probability=True)\n",
        "SVM_ovo_d_l.fit(x_left_down_train,y_left_down_train)\n",
        "print(SVM_ovo_d_l.score(x_left_down_test,y_left_down_test))\n",
        "\n",
        "SVM_ovo_r_l = svm.SVC(probability=True)\n",
        "SVM_ovo_r_l.fit(x_left_right_train,y_left_right_train)\n",
        "print(SVM_ovo_r_l.score(x_left_right_test,y_left_right_test))\n",
        "\n",
        "SVM_ovm_u = svm.SVC(probability=True)\n",
        "SVM_ovm_u.fit(x_up_train,y_up_train)\n",
        "print(SVM_ovm_u.score(x_up_test,y_up_test))\n",
        "\n",
        "SVM_ovm_r = svm.SVC(probability=True)\n",
        "SVM_ovm_r.fit(x_right_train,y_right_train)\n",
        "print(SVM_ovm_r.score(x_right_test,y_right_test))\n",
        "\n",
        "SVM_ovm_d = svm.SVC(probability=True)\n",
        "SVM_ovm_d.fit(x_down_train,y_down_train)\n",
        "print(SVM_ovm_d.score(x_down_test,y_down_test))\n",
        "\n",
        "SVM_ovm_l = svm.SVC(probabi56.lity=True)\n",
        "SVM_ovm_l.fit(x_left_train,y_left_train)\n",
        "print(SVM_ovm_l.score(x_left_test,y_left_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxtC5xvLq_4X"
      },
      "source": [
        "multiclass_LR = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "multiclass_LR.fit(x_csp_space_train,y_csp_space_train)\n",
        "print(multiclass_LR.score(x_csp_space_test,y_csp_space_test))\n",
        "\n",
        "LR_binary_search_v_h = LogisticRegression()\n",
        "LR_binary_search_v_h.fit(x_vertical_horizontal_train,y_vertical_horizontal_train)\n",
        "print(LR_binary_search_v_h.score(x_vertical_horizontal_test,y_vertical_horizontal_test))\n",
        "\n",
        "LR_ovo_u_d = LogisticRegression()\n",
        "LR_ovo_u_d.fit(x_up_down_train,y_up_down_train)\n",
        "print(LR_ovo_u_d.score(x_up_down_test,y_up_down_test))\n",
        "\n",
        "LR_ovo_u_r = LogisticRegression()\n",
        "LR_ovo_u_r .fit(x_right_up_train,y_right_up_train)\n",
        "print(LR_ovo_u_r .score(x_right_up_test,y_right_up_test))\n",
        "\n",
        "LR_ovo_u_l = LogisticRegression()\n",
        "LR_ovo_u_l.fit(x_left_up_train,y_left_up_train)\n",
        "print(LR_ovo_u_l.score(x_left_up_test,y_left_up_test))\n",
        "\n",
        "LR_ovo_d_r = LogisticRegression()\n",
        "LR_ovo_d_r.fit(x_right_down_train,y_right_down_train)\n",
        "print(LR_ovo_d_r.score(x_right_down_test,y_right_down_test))\n",
        "\n",
        "LR_ovo_d_l = LogisticRegression()\n",
        "LR_ovo_d_l.fit(x_left_down_train,y_left_down_train)\n",
        "print(LR_ovo_d_l.score(x_left_down_test,y_left_down_test))\n",
        "\n",
        "LR_ovo_r_l = LogisticRegression()\n",
        "LR_ovo_r_l.fit(x_left_right_train,y_left_right_train)\n",
        "print(LR_ovo_r_l.score(x_left_right_test,y_left_right_test))\n",
        "\n",
        "LR_ovm_u = LogisticRegression()\n",
        "LR_ovm_u.fit(x_up_train,y_up_train)\n",
        "print(LR_ovm_u.score(x_up_test,y_up_test))\n",
        "\n",
        "LR_ovm_r = LogisticRegression()\n",
        "LR_ovm_r.fit(x_right_train,y_right_train)\n",
        "print(LR_ovm_r.score(x_right_test,y_right_test))\n",
        "\n",
        "LR_ovm_d = LogisticRegression()\n",
        "LR_ovm_d.fit(x_down_train,y_down_train)\n",
        "print(LR_ovm_d.score(x_down_test,y_down_test))\n",
        "\n",
        "LR_ovm_l = LogisticRegression()\n",
        "LR_ovm_l.fit(x_left_train,y_left_train)\n",
        "print(LR_ovm_l.score(x_left_test,y_left_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtR66_X0f5Vj"
      },
      "source": [
        "The topology of this CNN is slightly different to the CNN used in the analysis of the spatial imagery data. This is because of the different input dimension. Here the input has dimension 127 by 22 by 1 rather than 375 by 22 by 1. The comments in the other file still explain the code used to construct the different layers thoroughly so it should still be possible to follow the structure of this CNN without extensive commenting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m1q_TrqrDfd"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "#initialising neural network\n",
        "model = models.Sequential()\n",
        "#specifying topology of neural network\n",
        "model.add(layers.Conv2D(22, (11,1), (1,1),input_shape=(127,22,1),padding=\"valid\"))\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "model.add(layers.SpatialDropout2D(.5))\n",
        "model.add(layers.Conv2D(44, (1,9), (1,1),padding=\"valid\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "model.add(layers.MaxPooling2D((2, 1),(2,1),padding=\"valid\"))\n",
        "model.add(layers.Conv2D(88, (11,1), (1,1),padding=\"valid\"))\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "model.add(layers.SpatialDropout2D(.5))\n",
        "model.add(layers.MaxPooling2D((2, 1),(2,1),padding=\"valid\"))\n",
        "model.add(layers.Conv2D(88, (11,1), (1,1),padding=\"valid\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "model.add(layers.SpatialDropout2D(.5))\n",
        "model.add(layers.MaxPooling2D((2, 1),(2,1),padding=\"valid\"))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(4))\n",
        "model.add(layers.Softmax())\n",
        "#specifying training mechanism\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "#training neural network\n",
        "model.fit(np.array(x_train_nn_space),np.array(y_train_nn_space),epochs=500,validation_data=(np.array(x_test_nn_space), np.array(y_test_nn_space)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnXDunLsrFvd"
      },
      "source": [
        "LDA_binary_outputs=[]\n",
        "LDA_ovo_outputs=[]\n",
        "LDA_ovm_outputs=[]\n",
        "LDA_multiclass_outputs=[]\n",
        "SVM_binary_outputs=[]\n",
        "SVM_ovo_outputs=[]\n",
        "SVM_ovm_outputs=[]\n",
        "SVM_multiclass_outputs=[]\n",
        "LR_binary_outputs=[]\n",
        "LR_ovo_outputs=[]\n",
        "LR_ovm_outputs=[]\n",
        "LR_multiclass_outputs=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dol9yv2frHjE"
      },
      "source": [
        "for i in x_all_test:\n",
        "  current=feature_extraction(i,csp_multiclass)\n",
        "  LDA_multiclass_outputs.append(multiclass_LDA.predict([current]))\n",
        "  SVM_multiclass_outputs.append(multiclass_SVM.predict([current]))\n",
        "  LR_multiclass_outputs.append(multiclass_LR.predict([current]))\n",
        "print(accuracy(LDA_multiclass_outputs,y_all_test))\n",
        "print(accuracy(SVM_multiclass_outputs,y_all_test))\n",
        "print(accuracy(LR_multiclass_outputs,y_all_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba0wXcrKrKAc"
      },
      "source": [
        "for i in x_all_test:\n",
        "  current_1=feature_extraction(i,csp_filters_up_right)\n",
        "  current_2=feature_extraction(i,csp_filters_up_down)\n",
        "  current_3=feature_extraction(i,csp_filters_up_left)\n",
        "  current_4=feature_extraction(i,csp_filters_down_right)\n",
        "  current_5=feature_extraction(i,csp_filters_right_left)\n",
        "  current_6=feature_extraction(i,csp_filters_down_left)\n",
        "  output_1=LDA_ovo_u_r.predict([current_1])\n",
        "  output_2=LDA_ovo_u_d.predict([current_2])\n",
        "  output_3=LDA_ovo_u_l.predict([current_3])\n",
        "  output_4=LDA_ovo_d_r.predict([current_4])\n",
        "  output_5=LDA_ovo_r_l.predict([current_5])\n",
        "  output_6=LDA_ovo_d_l.predict([current_6])\n",
        "  LDA_ovo_outputs.append(one_v_one([output_1,output_2,output_3,output_4,output_5,output_6]))\n",
        "  output_1=SVM_ovo_u_r.predict([current_1])\n",
        "  output_2=SVM_ovo_u_d.predict([current_2])\n",
        "  output_3=SVM_ovo_u_l.predict([current_3])\n",
        "  output_4=SVM_ovo_d_r.predict([current_4])\n",
        "  output_5=SVM_ovo_r_l.predict([current_5])\n",
        "  output_6=SVM_ovo_d_l.predict([current_6])\n",
        "  SVM_ovo_outputs.append(one_v_one([output_1,output_2,output_3,output_4,output_5,output_6]))\n",
        "  output_1=LR_ovo_u_r.predict([current_1])\n",
        "  output_2=LR_ovo_u_d.predict([current_2])\n",
        "  output_3=LR_ovo_u_l.predict([current_3])\n",
        "  output_4=LR_ovo_d_r.predict([current_4])\n",
        "  output_5=LR_ovo_r_l.predict([current_5])\n",
        "  output_6=LR_ovo_d_l.predict([current_6])\n",
        "  LR_ovo_outputs.append(one_v_one([output_1,output_2,output_3,output_4,output_5,output_6]))\n",
        "\n",
        "print(accuracy(LDA_ovo_outputs,y_all_test))\n",
        "print(accuracy(SVM_ovo_outputs,y_all_test))\n",
        "print(accuracy(LR_ovo_outputs,y_all_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k27PX6-OrM95"
      },
      "source": [
        "for i in x_all_test:\n",
        "  current_1=feature_extraction(i,csp_filters_v_h)\n",
        "  current_2=feature_extraction(i,csp_filters_up_down)\n",
        "  current_3=feature_extraction(i,csp_filters_right_left)\n",
        "  output_1=LDA_binary_search_v_h.predict([current_1])\n",
        "  output_2=LDA_ovo_u_d.predict([current_2])\n",
        "  output_3=LDA_ovo_r_l.predict([current_3])\n",
        "  LDA_binary_outputs.append(binary_search([output_1,output_2,output_3]))\n",
        "  output_1=SVM_binary_search_v_h.predict([current_1])\n",
        "  output_2=SVM_ovo_u_d.predict([current_2])\n",
        "  output_3=SVM_ovo_r_l.predict([current_3])\n",
        "  SVM_binary_outputs.append(binary_search([output_1,output_2,output_3]))\n",
        "  output_1=LR_binary_search_v_h.predict([current_1])\n",
        "  output_2=LR_ovo_u_d.predict([current_2])\n",
        "  output_3=LR_ovo_r_l.predict([current_3])\n",
        "  LR_binary_outputs.append(binary_search([output_1,output_2,output_3]))\n",
        "print(accuracy(LDA_binary_outputs,y_all_test))\n",
        "print(accuracy(SVM_binary_outputs,y_all_test))\n",
        "print(accuracy(LR_binary_outputs,y_all_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPk4rCO7rN1b"
      },
      "source": [
        "for i in x_all_test:\n",
        "  current_1=feature_extraction(i,csp_filters_up)\n",
        "  current_2=feature_extraction(i,csp_filters_down)\n",
        "  current_3=feature_extraction(i,csp_filters_left)\n",
        "  current_4=feature_extraction(i,csp_filters_right)\n",
        "  output_1=(LDA_ovm_u.predict_proba([current]))[0][0]\n",
        "  output_2=(LDA_ovm_d.predict_proba([current]))[0][0]\n",
        "  output_3=(LDA_ovm_l.predict_proba([current]))[0][0]\n",
        "  output_4=(LDA_ovm_r.predict_proba([current]))[0][0]\n",
        "  LDA_ovm_outputs.append(one_v_many([output_1,output_4,output_2,output_3]))\n",
        "  output_1=(SVM_ovm_u.predict_proba([current]))[0][0]\n",
        "  output_2=(SVM_ovm_d.predict_proba([current]))[0][0]\n",
        "  output_3=(SVM_ovm_l.predict_proba([current]))[0][0]\n",
        "  output_4=(SVM_ovm_r.predict_proba([current]))[0][0]\n",
        "  SVM_ovm_outputs.append(one_v_many([output_1,output_4,output_2,output_3]))\n",
        "  output_1=(LR_ovm_u.predict_proba([current]))[0][0]\n",
        "  output_2=(LR_ovm_d.predict_proba([current]))[0][0]\n",
        "  output_3=(LR_ovm_l.predict_proba([current]))[0][0]\n",
        "  output_4=(LR_ovm_r.predict_proba([current]))[0][0]\n",
        "  LR_ovm_outputs.append(one_v_many([output_1,output_4,output_2,output_3]))\n",
        "print(accuracy(LDA_ovm_outputs,y_all_test))\n",
        "print(accuracy(SVM_ovm_outputs,y_all_test))\n",
        "print(accuracy(LR_ovm_outputs,y_all_test))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}