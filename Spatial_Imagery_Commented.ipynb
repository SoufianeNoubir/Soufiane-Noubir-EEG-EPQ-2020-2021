{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spatial Imagery Commented.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYfSbOekcDVWPa/u82jQEj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoufianeNoubir/Soufiane-Noubir-EEG-EPQ-2020-2021/blob/main/Spatial_Imagery_Commented.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb3aghPVXkxH"
      },
      "source": [
        "Installing library for GDF files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc8Hhq4Lg_za",
        "outputId": "56a8b425-39e5-48c6-a82c-e5dade2c1ed6"
      },
      "source": [
        "pip install mne\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mne\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/29/7f38c7c99ca65fe4aac9054239d885c44ab7f9e8b4f65e9f2bfa489b0f38/mne-0.22.1-py3-none-any.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from mne) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from mne) (1.4.1)\n",
            "Installing collected packages: mne\n",
            "Successfully installed mne-0.22.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGcmHrvci07X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP_AyoOEdPWZ"
      },
      "source": [
        "Mounting Google drive to colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDCbs8bN0Fq_",
        "outputId": "95060006-135e-4500-c15d-ecbe8270340e"
      },
      "source": [
        "#Mounting Google drive to colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POlU9eEcdgxn"
      },
      "source": [
        "Loading in the relevant files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uCj7HOTgPpF",
        "outputId": "6d80fa68-3869-4e51-cd9d-f317d4d100f2"
      },
      "source": [
        "cd /content/drive/My Drive/EPQ datasets/BCICIV_2a_gdf.zip (Unzipped Files)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/EPQ datasets/BCICIV_2a_gdf.zip (Unzipped Files)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aGZjeuZgP3p",
        "outputId": "7833c22a-6560-4b1f-d49e-83a0cd52e667"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/EPQ datasets/BCICIV_2a_gdf.zip (Unzipped Files)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9H5OTvDdzci"
      },
      "source": [
        "Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeJ_bn6igdLg"
      },
      "source": [
        "#Importing the libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import mne\n",
        "from scipy.linalg import eigh\n",
        "import math\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OoAVJEedxz1"
      },
      "source": [
        "Loading the dataset from the files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xUSKwQQdw5N",
        "outputId": "a983d6df-6a41-4869-d782-df4c5d52c65f"
      },
      "source": [
        "#Loading the dataset from the files\n",
        "#Due to time limitations described in the report I will only be using one participant whose data is in the file \"A01T.gdf\"\n",
        "#This participant used an EEG with a sample rate of 200Hz.\n",
        "raw=mne.io.read_raw_gdf('A01T.gdf')\n",
        "#here we are extracting the actual EGG data from raw\n",
        "data=raw.get_data()\n",
        "#here we are extracting the labels from the dataset. \n",
        "#There is a label indicating when there is a change of state i.e. when the participant stops doing a certain action and starts another\n",
        "#Each label is a number which indicates the activity the participant has just started. I am only interested in the labels for imagining the tongue,feet,left and right arms.\n",
        "events_A, event_id_A = mne.events_from_annotations(raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting EDF parameters from /content/drive/My Drive/EPQ datasets/BCICIV_2a_gdf.zip (Unzipped Files)/A01T.gdf...\n",
            "GDF file detected\n",
            "Setting channel info structure...\n",
            "Creating raw.info structure...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/mne/io/edf/edf.py:1044: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
            "  etmode = np.fromstring(etmode, UINT8).tolist()[0]\n",
            "<ipython-input-7-b279547f86f7>:4: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
            "  raw=mne.io.read_raw_gdf('A01T.gdf')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n96mt-zDh-fk"
      },
      "source": [
        "This code snippet finds the the start points of all the relevant labels and saves the next 3 seconds(according to the documentation of this dataset each trial is 3 seconds) worth of data in the relevant dataset. Some of the data is invalid and these are marked by the reject label.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir3X-rG6BFOd"
      },
      "source": [
        "#splitting the dataset up by label.\n",
        "#This code snippet finds the the start points of all the relevant labels.\n",
        "#Some of the data is invalid and these are marked by the reject label.\n",
        "#The position of the relevant labels is scaled down by a factor of 2 as later the dataset will be downsampled to 125Hz.\n",
        "#The imagination of the tongue represents moving the cursor up.\n",
        "#The imagination of the feet represents moving the cursor down.\n",
        "#The imagination of the right arm represents moving the cursor to the right.\n",
        "#The imagination of the left arm represents moving the cursor to the left.\n",
        "rejections=[]\n",
        "left=[]\n",
        "right=[]\n",
        "up=[]\n",
        "down=[]\n",
        "\n",
        "for i in events_A:\n",
        "  if i[2]==1:\n",
        "    if i[0]%2==0:\n",
        "      rejections.append((i[0])/2)\n",
        "    else:\n",
        "      rejections.append(((i[0])+1)/2)\n",
        "  if i[2]==7:\n",
        "    if i[0]%2==0:\n",
        "      left.append((i[0])/2)\n",
        "    else:\n",
        "      left.append(((i[0])+1)/2)\n",
        "  if i[2]==8:\n",
        "    if i[0]%2==0:\n",
        "      right.append((i[0])/2)\n",
        "    else:\n",
        "      right.append(((i[0])+1)/2)\n",
        "  if i[2]==10:\n",
        "    if i[0]%2==0:\n",
        "      up.append((i[0])/2)\n",
        "    else:\n",
        "      up.append(((i[0])+1)/2)\n",
        "  if i[2]==9:\n",
        "    if i[0]%2==0:\n",
        "      down.append((i[0])/2)\n",
        "    else:\n",
        "      down.append(((i[0])+1)/2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHmChWxX7G8C"
      },
      "source": [
        "Function that removes unwanted electrodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz9pY--wJv4S"
      },
      "source": [
        "#removes unwanted electrodes\n",
        "#As discussed in report we only want the first 22 electrodes and thus the last 3 are omitted from our data.\n",
        "def remove_electrodes(data,numbers):\n",
        "  output=[]\n",
        "  for i in range(len(data)):\n",
        "    if i in numbers:\n",
        "      \n",
        "      output.append(data[i])\n",
        "  \n",
        "  return output\n",
        "data=remove_electrodes(data,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEbMx3T2kgH0"
      },
      "source": [
        "Function that downsamples by  a factor of 2 from 250Hz to 125Hz by only using using every second value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxLrJoa3NFG8"
      },
      "source": [
        "#downsamples data by a given factor\n",
        "def downsample(data,factor):\n",
        "  output=[]\n",
        "  current=[]\n",
        "  for i in data:\n",
        "    for j in range(len(i)):\n",
        "      if j%factor==0:\n",
        "        current.append(i[j])\n",
        "    output.append(current)\n",
        "    current=[]\n",
        "  return output\n",
        "#here we are downsampling by a factor of 2\n",
        "data_downsample=downsample(data,2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZJMVfJdXYuI"
      },
      "source": [
        "Splitting up the data into 4 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLz-70QcNI3-"
      },
      "source": [
        "#splits data up into segments of EEG data corresponding to one of the 4 classes\n",
        "#this is done by using the positions of each of the trials and extracting the relevant 3 seconds of EEG data if the trial is not a reject.\n",
        "\n",
        "#up_data contains the downsampled data for moving the cursor up.\n",
        "up_data=[]\n",
        "#down_data contains the downsampled data for moving the cursor down.\n",
        "down_data=[]\n",
        "#left_data contains the downsampled data for moving the cursor left.\n",
        "left_data=[]\n",
        "#right_data contains the downsampled data for moving the cursor right.\n",
        "right_data=[]\n",
        "current=[]\n",
        "for i in up:\n",
        "  if (int(i)-250) not in rejections:\n",
        "    for j in data_downsample:\n",
        "      current.append(j[int(i)+125:int(i)+500])\n",
        "    up_data.append(current)\n",
        "    current=[]\n",
        "for i in down:\n",
        "  if (int(i)-250) not in rejections:\n",
        "    for j in data_downsample:\n",
        "      current.append(j[int(i)+125:int(i)+500])\n",
        "    down_data.append(current)\n",
        "    current=[]\n",
        "for i in right:\n",
        "  if (int(i)-250) not in rejections:\n",
        "    for j in data_downsample:\n",
        "      current.append(j[int(i)+125:int(i)+500])\n",
        "    right_data.append(current)\n",
        "    current=[]\n",
        "for i in left:\n",
        "  if (int(i)-250) not in rejections:\n",
        "    for j in data_downsample:\n",
        "      current.append(j[int(i)+125:int(i)+500])\n",
        "    left_data.append(current)\n",
        "    current=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZR7ML4sXtSx"
      },
      "source": [
        "Splitting the data into training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xucARZbFNIrO"
      },
      "source": [
        "#splitting of dataset into training and testing data\n",
        "\n",
        "#left_data_evaluation contains the test data for moving the cursor left and so on\n",
        "#datapoints from left_data is removed and placed into left_data_evaluation with a probability of 1/4. \n",
        "#Therefore, overall the models will be trained on 3/4 the size of initial dataset and tested on the other quarter.\n",
        "#same is done for all 4 classes\n",
        "left_data_evaluation=[]\n",
        "right_data_evaluation=[]\n",
        "down_data_evaluation=[]\n",
        "up_data_evaluation=[]\n",
        "import random\n",
        "for i in up_data:\n",
        "  x=random.randint(1,4)\n",
        "  if x==4:\n",
        "    up_data.remove(i)\n",
        "    up_data_evaluation.append(i)\n",
        "for i in right_data:\n",
        "  x=random.randint(1,4)\n",
        "  if x==4:\n",
        "    right_data.remove(i)\n",
        "    right_data_evaluation.append(i)\n",
        "for i in down_data:\n",
        "  x=random.randint(1,4)\n",
        "  if x==4:\n",
        "    down_data.remove(i)\n",
        "    down_data_evaluation.append(i)\n",
        "for i in left_data:\n",
        "  x=random.randint(1,4)\n",
        "  if x==4:\n",
        "    left_data.remove(i)\n",
        "    left_data_evaluation.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53AHDKQU01v7"
      },
      "source": [
        "General data manipulation. Nothing very important or relevant is done here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lP_CGQR8EKw"
      },
      "source": [
        "\n",
        "#space_data is simply a collection of all the training data\n",
        "#It will be useful as it contains all the trials but are seperated by which class they belong to\n",
        "space_data=[]\n",
        "space_data.append(up_data)\n",
        "space_data.append(right_data)\n",
        "space_data.append(down_data)\n",
        "space_data.append(left_data)\n",
        "#space_data_evaluation is the exact same thing except for the training data\n",
        "space_data_evaluation=[]\n",
        "space_data_evaluation.append(up_data_evaluation)\n",
        "space_data_evaluation.append(right_data_evaluation)\n",
        "space_data_evaluation.append(down_data_evaluation)\n",
        "space_data_evaluation.append(left_data_evaluation)\n",
        "#x_all_test and y_all_test serve a similar purpose but have a slightly different\n",
        "#y_all_test contains the labels for the test data \n",
        "#class 1 is for up\n",
        "#class 2 is for right\n",
        "#class 3 is for down\n",
        "#class 4 is for left \n",
        "#we will be using this labelling of the 4 classes throughout the rest of the code.\n",
        "x_all_test=[]\n",
        "y_all_test=[]\n",
        "x_all_test+=up_data_evaluation\n",
        "x_all_test+=right_data_evaluation\n",
        "x_all_test+=down_data_evaluation\n",
        "x_all_test+=left_data_evaluation\n",
        "for i in up_data_evaluation:\n",
        "  y_all_test.append(1)\n",
        "for i in right_data_evaluation:\n",
        "  y_all_test.append(2)\n",
        "for i in down_data_evaluation:\n",
        "  y_all_test.append(3)\n",
        "for i in left_data_evaluation:\n",
        "  y_all_test.append(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqhZLS-LYExe"
      },
      "source": [
        "Implementation of standard CSP. CSP is an algorithm that as an input takes data that belongs to 2 different classes and finds filters that when the data is projected onto leaves a high variance in only one class compared to the other. This needs to be done for both classes i.e.(both classes need filters that will maximise the variance in that class compared to the other). This allows us to extract features that will help distinguish the 2 different classes. See report for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j024jS_NdmD"
      },
      "source": [
        "#implementation of standard CSP approach\n",
        "\n",
        "#data_1 and data_2 are datasets for 2 different classes on which we are trying to find the optimal set of filters that will distinguish between them\n",
        "\n",
        "#standard CSP can only take 2 different classes\n",
        "def csp(data_1,data_2):\n",
        "  #mean_cov_1 is the mean class covariance matrix for the first class\n",
        "  mean_cov_1=np.cov(np.array(data_1[0]))\n",
        "  #mean_cov_2 is the mean class covariance matrix for the second class\n",
        "  mean_cov_2=np.cov(np.array(data_2[0]))\n",
        " \n",
        "  for i in data_1[1:]:\n",
        "    #iterating through the first dataset and adding on the covariance matrix for each data point\n",
        "    mean_cov_1+=np.cov(np.array(i))\n",
        "    \n",
        "  for i in data_2[1:]:\n",
        "    #iterating through the second dataset and adding on the covariance matrix for each datapoint\n",
        "    mean_cov_2+=np.cov(np.array(i))\n",
        "  #here we divide by the number of datapoints in each class therefore producing our 2 mean class covariance matrices \n",
        "  mean_cov_1=mean_cov_1/len(data_1)\n",
        "  mean_cov_2=mean_cov_2/len(data_2)\n",
        "\n",
        "  #here we are generating the eigenvalues and eigenvectors of the matrix discussed in the appendix \n",
        "  #technically the matrix used here is different but the eigendecomposition of this matrix is mathematically equivalent to the one in the report and thus there is absolutely no difference\n",
        "  #the eigenvectors of this matrix is exactly the same as the eigenvectors of the matrix given in the report\n",
        "  #the eigenvalues will be different but the ordering of these eigenvalues will be the same\n",
        "  #i.e if you were to order the eigenvectors of this matrix in descending order of eigenvalue they would be in the same order as if you did the same with the matrix in the report\n",
        "  #thus the filters outputted will be the same\n",
        "  #the exact mathematical reasoning for this is not relevant\n",
        "  w_1, v_1 = np.linalg.eig(np.dot(np.linalg.inv(mean_cov_2+mean_cov_1),mean_cov_1))\n",
        "  sort=np.argsort(w_1)\n",
        "  #returning the eigenvectors correponding to the 3 smalles and largest eigenvalues\n",
        "  #these eigenvectors represent directions in space that when projected down onto leave a very high variance in one of the 2 classes and a very low variance in the other\n",
        "  #this allows us to distinguish between the 2 classes\n",
        "  #the difference between the vectors with the smallest and largest eigenvalues is which class the variance is being maximised in.\n",
        "  return ([v_1[sort[0]],v_1[sort[1]],v_1[sort[2]],v_1[sort[-3]],v_1[sort[-2]],v_1[sort[-1]]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI34i4uOZ6tE"
      },
      "source": [
        "Implementation of multiclass CSP. This is a generalisation of standard CSP that works on multiclass data. It is inspired by LDA. It finds filters that when the data is projected onto maximises the between class scatter to the within class scatter. This means that the spread between classes is large but each class forms its own small cluster making them easy to distinguish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zEnsT3jjJIT"
      },
      "source": [
        "#implementation of multiclass CSP\n",
        "\n",
        "# here data is a collection of data that belong to more than 2 classes i.e multiclass data.\n",
        "# as discussed in the report my generalisation of standard CSP is based on the mean scatter matrices rather than the mean covariance matrices.\n",
        "\n",
        "def multiclass_csp(data):\n",
        "  #this section of code is simply using the formula shown in the report to find the mean class scatter matrices\n",
        "\n",
        "  #total_elements finds the total number of datapoints in all classes\n",
        "  total_elements=0\n",
        "  for i in data:\n",
        "    total_elements+=len(i)\n",
        "  #average is a list that contains the class average for all classes\n",
        "  average=[]\n",
        "  for i in data:\n",
        "    total=np.array(i[0])\n",
        "    \n",
        "    for j in i[1:]:\n",
        "      \n",
        "      total+=np.array(j)\n",
        "    total=total/len(i)\n",
        "    \n",
        "      \n",
        "    average.append(total)\n",
        "  total_average=len(data[0])*average[0]\n",
        "  for i in range(len(data[1:])):\n",
        "    total_average+=len(data[i])*average[i]\n",
        "  total_average=total_average/total_elements\n",
        "  within_class=np.dot(data[0][0]-average[0],(data[0][0]-average[0]).transpose())\n",
        "  for i in range(len(data)):\n",
        "    \n",
        "    for j in data[i]:\n",
        "      within_class+=np.dot(j-average[i],(j-average[i]).transpose())\n",
        "  within_class-=np.dot(data[0][0]-average[0],(data[0][0]-average[0]).transpose())\n",
        "  #this scaling of the within_class scatter matrix was simply to make the numbers more manageable during debugging and has no mathematical significance. \n",
        "  within_class=within_class/total_elements\n",
        "  between_class=len(data[0])*(np.dot(average[0]-total_average,(average[0]-total_average).transpose()))\n",
        "  for i in range(len(average[1:])):\n",
        "    between_class+=len(data[i])*(np.dot(average[i]-total_average,(average[i]-total_average).transpose()))\n",
        "  #this scaling of the between_class scatter matrix was simply to make the numbers more manageable during debugging and has no mathematical significance.\n",
        "  between_class=between_class/len(data)\n",
        "\n",
        "\n",
        "  #here we are generating the eigenvalues and eigenvectors of the matrix discussed in the appendix section for LDA.\n",
        "  #technically the matrix used here is different but the eigendecomposition of this matrix is mathematically equivalent to the one in the report and thus there is absolutely no difference\n",
        "  #the eigenvectors of this matrix is exactly the same as the eigenvectors of the matrix given in the report\n",
        "  #the eigenvalues will be different but the ordering of these eigenvalues will be the same\n",
        "  #i.e if you were to order the eigenvectors of this matrix in descending order of eigenvalue they would be in the same order as if you did the same with the matrix in the report\n",
        "  #thus the filters outputted will be the same\n",
        "  #the exact mathematical reasoning behind why both the eigendecompostions are exactly the same is not relevant\n",
        "  #the reason I am using this slightly adapted matrix is because it is easier to decompose\n",
        "  w_1, v_1 = np.linalg.eig(np.dot(np.linalg.inv(between_class+within_class),between_class))\n",
        "  sort=np.argsort(w_1)\n",
        "  #here since we are trying to maximise the ratio of the between class scatter to the within class scatter we do not consider the eigenvectors with the smallest eigenvalues\n",
        "  return ([v_1[sort[0]],v_1[sort[1]],v_1[sort[2]],v_1[sort[3]],v_1[sort[4]],v_1[sort[5]]])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6FgUeHyFFSV"
      },
      "source": [
        "Feature extraction. This algorithm takes the filters produced by CSP and uses them to extract feautres that represent the data which a classifier can classify. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2El8eEQV5uzp"
      },
      "source": [
        "#implementation of feature extraction using filters generated from CSP\n",
        "#here the input is up_data which is simply a matrix that represents a single trial of EEG data\n",
        "#the filters are all in turn being applied to this matrix resulting in a vector each time\n",
        "#vectors are arrays of number which we simply take the variance of\n",
        "#these variances are the features that will be used for classification\n",
        "def feature_extraction(up_data,filters):\n",
        "  features=[]\n",
        "  for i in filters:\n",
        "    #np.dot() function applies the filter to the data\n",
        "    #np.var() function takes the variance of the resulting vector\n",
        "    #np\n",
        "    features.append(math.log(np.var(np.dot(i.transpose(),up_data))))\n",
        "    \n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3zbzO-vKr9G"
      },
      "source": [
        "Generating the filters for all the CSP based classification we will be doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWr31f8v57tV"
      },
      "source": [
        "#the 3 different CSP classification approaches are describes in detail in the report\n",
        "\n",
        "#one of them is based off OvM classification\n",
        "#this consists of training 4 binary classifiers to distinguish between up and the rest, down and the rest, right and rest, left and rest\n",
        "#for example csp_filters_left are filters that will distinguish between left versus up, down and right\n",
        "csp_filters_left=csp(left_data,right_data+up_data+down_data)\n",
        "csp_filters_right=csp(right_data,left_data+up_data+down_data)\n",
        "csp_filters_down=csp(down_data,right_data+up_data+left_data)\n",
        "csp_filters_up=csp(up_data,right_data+left_data+down_data)\n",
        "\n",
        "#the other was based on OvO classification\n",
        "#this is based on training 6 binary classifiers to distinguish between every pair of classes i.e up v down, right v up etc..\n",
        "#for example csp_filters_up_down are filters that will distinguish between up and down\n",
        "csp_filters_up_down=csp(up_data,down_data)\n",
        "csp_filters_down_right=csp(right_data,down_data)\n",
        "csp_filters_up_right=csp(up_data,right_data)\n",
        "csp_filters_down_left=csp(left_data,down_data)\n",
        "csp_filters_right_left=csp(right_data,left_data)\n",
        "csp_filters_up_left=csp(up_data,left_data)\n",
        "#the final csp based approach was binary search which first requires distinguishing between vertical and horizontal directions\n",
        "#csp_filters_v_h will distinguish between the up/down direction and the right/left direction\n",
        "#once this classification is done then we do a up v down classification or a right v left classification.\n",
        "csp_filters_v_h=csp(up_data+down_data,right_data+left_data)\n",
        "#csp_multiclass are filters that are generated using my multiclass generalisation of CSP that will distinguish between all 4 classes \n",
        "csp_multiclass=multiclass_csp(space_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSEwxeo4SoJD"
      },
      "source": [
        "This section of code is initialising all the datasets for all the classifications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn5cdG7Z58JL"
      },
      "source": [
        "x_vertical_horizontal_train=[]\n",
        "y_vertical_horizontal_train=[]\n",
        "x_vertical_horizontal_test=[]\n",
        "y_vertical_horizontal_test=[]\n",
        "x_up_down_train=[]\n",
        "y_up_down_train=[]\n",
        "x_up_down_test=[]\n",
        "y_up_down_test=[]\n",
        "x_left_right_train=[]\n",
        "y_left_right_train=[]\n",
        "x_left_right_test=[]\n",
        "y_left_right_test=[]\n",
        "x_left_train=[]\n",
        "y_left_train=[]\n",
        "x_left_test=[]\n",
        "y_left_test=[]\n",
        "x_right_train=[]\n",
        "y_right_train=[]\n",
        "x_right_test=[]\n",
        "y_right_test=[]\n",
        "x_up_train=[]\n",
        "y_up_train=[]\n",
        "x_up_test=[]\n",
        "y_up_test=[]\n",
        "x_down_train=[]\n",
        "y_down_train=[]\n",
        "x_down_test=[]\n",
        "y_down_test=[]\n",
        "x_right_up_train=[]\n",
        "y_right_up_train=[]\n",
        "x_right_up_test=[]\n",
        "y_right_up_test=[]\n",
        "x_right_down_train=[]\n",
        "y_right_down_train=[]\n",
        "x_right_down_test=[]\n",
        "y_right_down_test=[]\n",
        "x_left_up_train=[]\n",
        "y_left_up_train=[]\n",
        "x_left_up_test=[]\n",
        "y_left_up_test=[]\n",
        "x_left_down_train=[]\n",
        "y_left_down_train=[]\n",
        "x_left_down_test=[]\n",
        "y_left_down_test=[]\n",
        "x_csp_space_train=[]\n",
        "y_csp_space_train=[]\n",
        "x_csp_space_test=[]\n",
        "y_csp_space_test=[]\n",
        "x_train_nn_space=[]\n",
        "y_train_nn_space=[]\n",
        "x_test_nn_space=[]\n",
        "y_test_nn_space=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-JxeuS-TSjy"
      },
      "source": [
        "This section of code is actually creating the datasets. It is very repetitive so I will explain the code for only a few datasets and the rest is self-explanatory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oewp1Mgo59Gv"
      },
      "source": [
        "#the x_vertical_horizontal_train is a dataset which contains the features for distinguishing between the vertical and horizontal directions that will be used for training\n",
        "#the dataset is generated by iterating through the all trials in all the 4 classes \n",
        "#then the feature_extraction function is applied to each individual trial using csp_filters_v_h which are filters which distiniguish beetween horizontal and vertical directions\n",
        "for i in up_data:\n",
        "  x_vertical_horizontal_train.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in down_data:\n",
        "  x_vertical_horizontal_train.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in right_data:\n",
        "  x_vertical_horizontal_train.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in left_data:\n",
        "  x_vertical_horizontal_train.append(feature_extraction(i,csp_filters_v_h))\n",
        "#the exact same is done for x_vertical_horizontal_test which is the dataset that will be used for testing\n",
        "for i in up_data_evaluation:\n",
        "  x_vertical_horizontal_test.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in down_data_evaluation:\n",
        "  x_vertical_horizontal_test.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in right_data_evaluation:\n",
        "  x_vertical_horizontal_test.append(feature_extraction(i,csp_filters_v_h))\n",
        "for i in left_data_evaluation:\n",
        "  x_vertical_horizontal_test.append(feature_extraction(i,csp_filters_v_h))\n",
        "#the x_up_down_train is a dataset which contains the features for distinguishing between the up and down directions that will be used for training\n",
        "#the dataset is generated by iterating through the all trials in all the up and down classes\n",
        "#then the feature_extraction function is applied to each individual trial using csp_filters_up_down which are filters which distiniguish beetween up and down directions\n",
        "for i in up_data:\n",
        "  x_up_down_train.append(feature_extraction(i,csp_filters_up_down))\n",
        "for i in down_data:\n",
        "  x_up_down_train.append(feature_extraction(i,csp_filters_up_down))\n",
        "#the exact same is done for x_up_down_test which is the dataset that will be used for testing\n",
        "for i in up_data_evaluation:\n",
        "  x_up_down_test.append(feature_extraction(i,csp_filters_up_down))\n",
        "for i in down_data_evaluation:\n",
        "  x_up_down_test.append(feature_extraction(i,csp_filters_up_down))\n",
        "\n",
        "for i in right_data:\n",
        "  x_left_right_train.append(feature_extraction(i,csp_filters_right_left))\n",
        "for i in left_data:\n",
        "  x_left_right_train.append(feature_extraction(i,csp_filters_right_left))\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  x_left_right_test.append(feature_extraction(i,csp_filters_right_left))\n",
        "for i in left_data_evaluation:\n",
        "  x_left_right_test.append(feature_extraction(i,csp_filters_right_left))\n",
        "\n",
        "for i in right_data:\n",
        "  x_right_up_train.append(feature_extraction(i,csp_filters_up_right))\n",
        "for i in up_data:\n",
        "  x_right_up_train.append(feature_extraction(i,csp_filters_up_right))\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  x_right_up_test.append(feature_extraction(i,csp_filters_up_right))\n",
        "for i in up_data_evaluation:\n",
        "  x_right_up_test.append(feature_extraction(i,csp_filters_up_right))\n",
        "\n",
        "for i in right_data:\n",
        "  x_right_down_train.append(feature_extraction(i,csp_filters_down_right))\n",
        "for i in down_data:\n",
        "  x_right_down_train.append(feature_extraction(i,csp_filters_down_right))\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  x_right_down_test.append(feature_extraction(i,csp_filters_down_right))\n",
        "for i in down_data_evaluation:\n",
        "  x_right_down_test.append(feature_extraction(i,csp_filters_down_right))\n",
        "\n",
        "for i in left_data:\n",
        "  x_left_up_train.append(feature_extraction(i,csp_filters_up_left))\n",
        "for i in up_data:\n",
        "  x_left_up_train.append(feature_extraction(i,csp_filters_up_left))\n",
        "\n",
        "for i in left_data_evaluation:\n",
        "  x_left_up_test.append(feature_extraction(i,csp_filters_up_left))\n",
        "for i in up_data_evaluation:\n",
        "  x_left_up_test.append(feature_extraction(i,csp_filters_up_left))\n",
        "\n",
        "\n",
        "for i in left_data:\n",
        "  x_left_down_train.append(feature_extraction(i,csp_filters_down_left))\n",
        "for i in down_data:\n",
        "  x_left_down_train.append(feature_extraction(i,csp_filters_down_left))\n",
        "\n",
        "for i in left_data_evaluation:\n",
        "  x_left_down_test.append(feature_extraction(i,csp_filters_down_left))\n",
        "for i in down_data_evaluation:\n",
        "  x_left_down_test.append(feature_extraction(i,csp_filters_down_left))\n",
        "\n",
        "\n",
        "for i in up_data:\n",
        "  x_left_train.append(feature_extraction(i,csp_filters_left))\n",
        "for i in down_data:\n",
        "  x_left_train.append(feature_extraction(i,csp_filters_left))\n",
        "for i in right_data:\n",
        "  x_left_train.append(feature_extraction(i,csp_filters_left))\n",
        "for i in left_data:\n",
        "  x_left_train.append(feature_extraction(i,csp_filters_left))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_left_test.append(feature_extraction(i,csp_filters_left))\n",
        "for i in down_data_evaluation:\n",
        "  x_left_test.append(feature_extraction(i,csp_filters_left))\n",
        "for i in right_data_evaluation:\n",
        "  x_left_test.append(feature_extraction(i,csp_filters_left))\n",
        "for i in left_data_evaluation:\n",
        "  x_left_test.append(feature_extraction(i,csp_filters_left))\n",
        "\n",
        "for i in up_data:\n",
        "  x_right_train.append(feature_extraction(i,csp_filters_right))\n",
        "for i in down_data:\n",
        "  x_right_train.append(feature_extraction(i,csp_filters_right))\n",
        "for i in right_data:\n",
        "  x_right_train.append(feature_extraction(i,csp_filters_right))\n",
        "for i in left_data:\n",
        "  x_right_train.append(feature_extraction(i,csp_filters_right))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_right_test.append(feature_extraction(i,csp_filters_right))\n",
        "for i in down_data_evaluation:\n",
        "  x_right_test.append(feature_extraction(i,csp_filters_right))\n",
        "for i in right_data_evaluation:\n",
        "  x_right_test.append(feature_extraction(i,csp_filters_right))\n",
        "for i in left_data_evaluation:\n",
        "  x_right_test.append(feature_extraction(i,csp_filters_right))\n",
        "\n",
        "for i in up_data:\n",
        "  x_up_train.append(feature_extraction(i,csp_filters_up))\n",
        "for i in down_data:\n",
        "  x_up_train.append(feature_extraction(i,csp_filters_up))\n",
        "for i in right_data:\n",
        "  x_up_train.append(feature_extraction(i,csp_filters_up))\n",
        "for i in left_data:\n",
        "  x_up_train.append(feature_extraction(i,csp_filters_up))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_up_test.append(feature_extraction(i,csp_filters_up))\n",
        "for i in down_data_evaluation:\n",
        "  x_up_test.append(feature_extraction(i,csp_filters_up))\n",
        "for i in right_data_evaluation:\n",
        "  x_up_test.append(feature_extraction(i,csp_filters_up))\n",
        "for i in left_data_evaluation:\n",
        "  x_up_test.append(feature_extraction(i,csp_filters_up))\n",
        "\n",
        "for i in up_data:\n",
        "  x_down_train.append(feature_extraction(i,csp_filters_down))\n",
        "for i in down_data:\n",
        "  x_down_train.append(feature_extraction(i,csp_filters_down))\n",
        "for i in right_data:\n",
        "  x_down_train.append(feature_extraction(i,csp_filters_down))\n",
        "for i in left_data:\n",
        "  x_down_train.append(feature_extraction(i,csp_filters_down))\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  x_down_test.append(feature_extraction(i,csp_filters_down))\n",
        "for i in down_data_evaluation:\n",
        "  x_down_test.append(feature_extraction(i,csp_filters_down))\n",
        "for i in right_data_evaluation:\n",
        "  x_down_test.append(feature_extraction(i,csp_filters_down))\n",
        "for i in left_data_evaluation:\n",
        "  x_down_test.append(feature_extraction(i,csp_filters_down))\n",
        "#x_csp_space_train is the dataset that will be used for multiclass classification to distinguish between all 4 classes at once\n",
        "#the dataset is generated by iterating through all trials in all 4 classes\n",
        "#then the feature_extraction function is applied to each individual trial using csp_space which are filters which distiniguish beetween all directions\n",
        "for i in up_data:\n",
        "  x_csp_space_train.append(feature_extraction(i,csp_multiclass))\n",
        "for i in down_data:\n",
        "  x_csp_space_train.append(feature_extraction(i,csp_multiclass))\n",
        "for i in right_data:\n",
        "  x_csp_space_train.append(feature_extraction(i,csp_multiclass))\n",
        "for i in left_data:\n",
        "  x_csp_space_train.append(feature_extraction(i,csp_multiclass))\n",
        "#the exact same is done for x_csp_space_test which is the dataset that will be used for testing\n",
        "for i in up_data_evaluation:\n",
        "  x_csp_space_test.append(feature_extraction(i,csp_multiclass))\n",
        "for i in down_data_evaluation:\n",
        "  x_csp_space_test.append(feature_extraction(i,csp_multiclass))\n",
        "for i in right_data_evaluation:\n",
        "  x_csp_space_test.append(feature_extraction(i,csp_multiclass))\n",
        "for i in left_data_evaluation:\n",
        "  x_csp_space_test.append(feature_extraction(i,csp_multiclass))\n",
        "\n",
        "#this section of code is for creating the dataset for the CNN\n",
        "#x_train_nn_space is the dataset for training the CNN\n",
        "#y_train_nn_space contains the correct labels for the x_train_nn_space\n",
        "for i in space_data:\n",
        "  \n",
        "  for j in i:\n",
        "    intermediate=np.transpose(j)\n",
        "    intermediate_3=[]\n",
        "    for k in intermediate:\n",
        "      intermediate_2=[]\n",
        "      for l in k:\n",
        "        intermediate_2.append(np.array([l]))\n",
        "      intermediate_3.append(np.array(intermediate_2))\n",
        "    x_train_nn_space.append(np.array(intermediate_3))\n",
        "    #to generate the data in x_train_space not much processing needs to be done\n",
        "    #firstly the matrix representing each trial is transposed so that it fits the dimension of the CNN that will be trained\n",
        "    #then since CNNs are usually applied to images with multiple channels(RGB channels) I need to turn each individual EEG reading into a list of length 1 to indicate there is only one channel\n",
        "    #therefore each matrix will be of dimension (375,22,1) 375 for the number of recordings by each electrode, 22 for the number of electrodes and 1 to indicate only one channel per electrode.\n",
        "    #The CNN will now be able to train on this 2 dimensional data without further processing\n",
        "#since the CNN will be trained using categorical cross entropy loss the correct labels should be lists of length 4\n",
        "#each element of the list gives the probability that the element is in each class\n",
        "#These probabilities will all be 0 except for a probability of 1 in the correct class\n",
        "#for example the correct label for a trial in the first class is [1,0,0,0]\n",
        "for i in up_data:\n",
        "  y_train_nn_space.append(np.array([1,0,0,0]))\n",
        "for i in right_data:\n",
        "  y_train_nn_space.append(np.array([0,1,0,0]))\n",
        "for i in down_data:\n",
        "  y_train_nn_space.append(np.array([0,0,1,0]))\n",
        "for i in left_data:\n",
        "  y_train_nn_space.append(np.array([0,0,0,1]))\n",
        "# the same is done for the test data and the labels for the test data  \n",
        "for i in space_data_evaluation:\n",
        "  \n",
        "  for j in i:\n",
        "    intermediate=np.transpose(j)\n",
        "    intermediate_3=[]\n",
        "    for k in intermediate:\n",
        "      intermediate_2=[]\n",
        "      for l in k:\n",
        "        intermediate_2.append(np.array([l]))\n",
        "      intermediate_3.append(np.array(intermediate_2))   \n",
        "    \n",
        "    x_test_nn_space.append(np.array(intermediate_3))\n",
        "    \n",
        "for i in up_data_evaluation:\n",
        "  y_test_nn_space.append(np.array([1,0,0,0]))\n",
        "for i in right_data_evaluation:\n",
        "  y_test_nn_space.append(np.array([0,1,0,0]))\n",
        "for i in down_data_evaluation:\n",
        "  y_test_nn_space.append(np.array([0,0,1,0]))\n",
        "for i in left_data_evaluation:\n",
        "  y_test_nn_space.append(np.array([0,0,0,1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozz1MSaqxBuW"
      },
      "source": [
        "Here the labels for the datasets for the CSP based approach are made. Like in the previous section the code is very repetitive and I will only explain a few."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVpLOW9Q6AgE"
      },
      "source": [
        "#y_vertical_horizontal_train contians the correct labels for x_horizontal_vertical_train\n",
        "#the label 1 is given for any direction in the vertical direction\n",
        "#the label 2 is given for any direction in the horizontal direction\n",
        "for i in up_data:\n",
        " y_vertical_horizontal_train.append(1)\n",
        "for i in down_data:\n",
        "  y_vertical_horizontal_train.append(1)\n",
        "for i in right_data:\n",
        "  y_vertical_horizontal_train.append(2)\n",
        "for i in left_data:\n",
        "  y_vertical_horizontal_train.append(2)\n",
        "#the same is done for y_vertical_horizontal_test which is the set of labels for the test data\n",
        "for i in up_data_evaluation:\n",
        "  y_vertical_horizontal_test.append(1)\n",
        "for i in down_data_evaluation:\n",
        "  y_vertical_horizontal_test.append(1)\n",
        "for i in right_data_evaluation:\n",
        "  y_vertical_horizontal_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_vertical_horizontal_test.append(2)\n",
        "#y_up_down_train contains the correct labels for x_up_down_train\n",
        "#the labelling system descrive earlier is used\n",
        "#the label 1 is given for the up direction\n",
        "#the label 3 is given for the down direction\n",
        "for i in up_data:\n",
        "  y_up_down_train.append(1)\n",
        "for i in down_data:\n",
        "  y_up_down_train.append(3)\n",
        "#the same is done for y_up_down_test which is the set of labels for the test data\n",
        "for i in up_data_evaluation:\n",
        "  y_up_down_test.append(1)\n",
        "for i in down_data_evaluation:\n",
        "  y_up_down_test.append(3)\n",
        "\n",
        "for i in right_data:\n",
        "  y_left_right_train.append(2)\n",
        "for i in left_data:\n",
        "  y_left_right_train.append(4)\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  y_left_right_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_left_right_test.append(4)\n",
        "\n",
        "for i in right_data:\n",
        "  y_right_up_train.append(2)\n",
        "for i in up_data:\n",
        "  y_right_up_train.append(1)\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  y_right_up_test.append(2)\n",
        "for i in up_data_evaluation:\n",
        "  y_right_up_test.append(1)\n",
        "\n",
        "for i in right_data:\n",
        "  y_right_down_train.append(2)\n",
        "for i in down_data:\n",
        "  y_right_down_train.append(3)\n",
        "\n",
        "for i in right_data_evaluation:\n",
        "  y_right_down_test.append(2)\n",
        "for i in down_data_evaluation:\n",
        "  y_right_down_test.append(3)\n",
        "\n",
        "for i in left_data:\n",
        "  y_left_up_train.append(4)\n",
        "for i in up_data:\n",
        "  y_left_up_train.append(1)\n",
        "\n",
        "for i in left_data_evaluation:\n",
        "  y_left_up_test.append(4)\n",
        "for i in up_data_evaluation:\n",
        "  y_left_up_test.append(1)\n",
        "\n",
        "for i in left_data:\n",
        "  y_left_down_train.append(4)\n",
        "for i in down_data:\n",
        "  y_left_down_train.append(3)\n",
        "\n",
        "for i in left_data_evaluation:\n",
        "  y_left_down_test.append(4)\n",
        "for i in down_data_evaluation:\n",
        "  y_left_down_test.append(3)\n",
        "\n",
        "for i in up_data:\n",
        "  y_left_train.append(2)\n",
        "for i in down_data:\n",
        "  y_left_train.append(2)\n",
        "for i in right_data:\n",
        "  y_left_train.append(2)\n",
        "for i in left_data:\n",
        "  y_left_train.append(1)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_left_test.append(2)\n",
        "for i in down_data_evaluation:\n",
        "  y_left_test.append(2)\n",
        "for i in right_data_evaluation:\n",
        "  y_left_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_left_test.append(1)\n",
        "\n",
        "for i in up_data:\n",
        "  y_right_train.append(2)\n",
        "for i in down_data:\n",
        "  y_right_train.append(2)\n",
        "for i in right_data:\n",
        "  y_right_train.append(1)\n",
        "for i in left_data:\n",
        "  y_right_train.append(2)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_right_test.append(2)\n",
        "for i in down_data_evaluation:\n",
        "  y_right_test.append(2)\n",
        "for i in right_data_evaluation:\n",
        "  y_right_test.append(1)\n",
        "for i in left_data_evaluation:\n",
        "  y_right_test.append(2)\n",
        "\n",
        "for i in up_data:\n",
        "  y_up_train.append(1)\n",
        "for i in down_data:\n",
        "  y_up_train.append(2)\n",
        "for i in right_data:\n",
        "  y_up_train.append(2)\n",
        "for i in left_data:\n",
        "  y_up_train.append(2)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_up_test.append(1)\n",
        "for i in down_data_evaluation:\n",
        "  y_up_test.append(2)\n",
        "for i in right_data_evaluation:\n",
        "  y_up_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_up_test.append(2)\n",
        "\n",
        "for i in up_data:\n",
        "  y_down_train.append(2)\n",
        "for i in down_data:\n",
        "  y_down_train.append(1)\n",
        "for i in right_data:\n",
        "  y_down_train.append(2)\n",
        "for i in left_data:\n",
        "  y_down_train.append(2)\n",
        "\n",
        "for i in up_data_evaluation:\n",
        "  y_down_test.append(2)\n",
        "for i in down_data_evaluation:\n",
        "  y_down_test.append(1)\n",
        "for i in right_data_evaluation:\n",
        "  y_down_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_down_test.append(2)\n",
        "#y_csp_space_train contians the correct labels for x_csp_space_train\n",
        "#the label 1 is given for going up\n",
        "#the label 2 is given for going right\n",
        "#the label 3 is given for going down\n",
        "#the label 4 is given for going left\n",
        "for i in up_data:\n",
        "  y_csp_space_train.append(1)\n",
        "for i in down_data:\n",
        "  y_csp_space_train.append(3)\n",
        "for i in right_data:\n",
        "  y_csp_space_train.append(2)\n",
        "for i in left_data:\n",
        "  y_csp_space_train.append(4)\n",
        "#the same is done for y_csp_space_test which is the set of labels for the test data\n",
        "for i in up_data_evaluation:\n",
        "  y_csp_space_test.append(1)\n",
        "for i in down_data_evaluation:\n",
        "  y_csp_space_test.append(3)\n",
        "for i in right_data_evaluation:\n",
        "  y_csp_space_test.append(2)\n",
        "for i in left_data_evaluation:\n",
        "  y_csp_space_test.append(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHpP6mZWG4nh"
      },
      "source": [
        "Implementation of the OvO and OvM functions. These take the relevant outputs from classifiers and uses the OvO and OvM generalisation to make a classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXE7ONAB6Gve"
      },
      "source": [
        "#uses one_v_one classification to output the predicted class\n",
        "#takes the output of 6 classifiers which distiniguish any 2 classes i.e up v down classifier, down v right classifier, up v right classifier etc.\n",
        "\n",
        "def one_v_one(outputs):\n",
        "  #counts the number of classifiers that output class 1,2,3,4\n",
        "  num_1=outputs.count(1)\n",
        "  num_2=outputs.count(2)\n",
        "  num_3=outputs.count(3)\n",
        "  num_4=outputs.count(4)\n",
        "  num=[num_1,num_2,num_3,num_4]\n",
        "  #the class with the most votes is selected\n",
        "  if num.count(max(num))==1:\n",
        "    return (num.index(max(num)))+1\n",
        "  \n",
        "  #if there is a 2 way tie the output of the classifier used to distinguish between the 2 leading classes is used\n",
        "  #for example if classes 1 and 2 got 2 votes and 3 and 4 each got one vote then the chosen class will be the output of the up v right classifier\n",
        "  else:\n",
        "    \n",
        "    winners=[]\n",
        "    for i in range(len(num)):\n",
        "      if num[i] ==max(num):\n",
        "        winners.append(i+1)\n",
        "    #as discussed in the report the chance of a 3 way tie is unlikely and I will not be considering this case\n",
        "    #however, if one does occur, the code will output an alert message to notify me\n",
        "    if len(winners)==3:\n",
        "      return (\"alert\")\n",
        "    else:\n",
        "      if min(winners)==1:\n",
        "        return outputs[sum(winners)-3]\n",
        "      else:\n",
        "        return outputs[sum(winners)-2]\n",
        "#uses one_v_many classification to output the predicted class\n",
        "#takes the output of 4 classifiers which distinguish between one class and rest i.e. up v right,down,left, right v up,down,left, down v up,right,left, left v up,right,down \n",
        "def one_v_many(probs):\n",
        "  #this functions takes in the confidence of each model in the individual class\n",
        "  #the confidence in class k is the posterior probability i.e P(y=k,x)\n",
        "  #the confidence tells us how likely it is that the input belongs to the one class\n",
        "  #for example the first value in the list probs will be the confidence of the up v right,down,left model in the up class\n",
        "  #the class whose model has the highest confidence in it is chosen\n",
        "  return (probs.index(max(probs))+1)\n",
        "#uses binary search to output the predicted class\n",
        "def binary_search(outputs):\n",
        "  #binary search works by distinguishing between vertical and horizontal directions\n",
        "  if outputs[0]==1:\n",
        "    #if the input is in the vetical direction then the output of the up v down classifier is used\n",
        "    return outputs[1]\n",
        "  else:\n",
        "    #if the input is in the horizontal direction then the output of the right v left classifier is used\n",
        "    return outputs[2]\n",
        "#returns accuracy\n",
        "def accuracy(outputs,correct):\n",
        "  #this is my custom accuracy function that takes then outputs of my classification algorithm and checks it againsts the correct labels for the test data.\n",
        "  right=0\n",
        "  for i in range(len(outputs)):\n",
        "    if outputs[i]==correct[i]:\n",
        "      right+=1\n",
        "  return right/(len(outputs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDaQfKwZhc86"
      },
      "source": [
        "Here I implement each of these approaches using LDA. OvO,OvM and bianry search appraoches are done using LDA. I use multiclass LDA for the features extracted by my multiclass CSP algorithm. LDA works by by fidning a projection that will maximise the ratio of the between class scatter to the within class scatter. The between class scatter gives a sense of how far apart differen classes are whereas within class scatter gives a sense of how spread out each individual class. If we maximise this ratio then it will become easier to distinguish between different classes. Classification using LDA is done by projecting onto this optimal filter and assigning the input to the class whose mean it is closest to. We can get the confidence in the each class by assuming each class is normally distributed with the same covariance matrix and then applying bayes formula. See appendix for details. Once again the code is very repetitive I will only comment a few sections and the rest is self explanatory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUhTcyS46Mlp"
      },
      "source": [
        "#this secction creates the muticlass LDA model that will classify features extracted using my multiclass CSP algorithm\n",
        "multiclass_LDA = LinearDiscriminantAnalysis()\n",
        "#it is being trained on x_csp_space_train with the labels y_csp_space_train\n",
        "multiclass_LDA.fit(x_csp_space_train,y_csp_space_train)\n",
        "#here I am evaluating the accuracy of the model on the test data c_csp_space_test with the labels y_csp_space_test\n",
        "print(multiclass_LDA.score(x_csp_space_test,y_csp_space_test))\n",
        "\n",
        "#this section creates the LDA model that will classify features extracted using standard CSP.\n",
        "#the model will distinguish between the vertical and horizontal directions\n",
        "LDA_binary_search_v_h = LinearDiscriminantAnalysis()\n",
        "#it is being trained on x_vertical_horizontal_train with the labels y_vertical_horizontal_train\n",
        "LDA_binary_search_v_h.fit(x_vertical_horizontal_train,y_vertical_horizontal_train)\n",
        "#here I am evaluating the accuracy of the model on the test data x_vertical_horizontal_test with the labels y_vertical_horizontal_test\n",
        "print(LDA_binary_search_v_h.score(x_vertical_horizontal_test,y_vertical_horizontal_test))\n",
        "\n",
        "#this section creates the LDA model that will classify features extracted using standard CSP.\n",
        "#the model will distinguish between up and down\n",
        "LDA_ovo_u_d = LinearDiscriminantAnalysis()\n",
        "#it is being trained on x_up_down_train with the labels y_up_down_train\n",
        "LDA_ovo_u_d.fit(x_up_down_train,y_up_down_train)\n",
        "#here I am evaluating the accuracy of the model on the test data x_up_down_test with the labels y_up_down_test\n",
        "print(LDA_ovo_u_d.score(x_up_down_test,y_up_down_test))\n",
        "\n",
        "LDA_ovo_u_r = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_u_r .fit(x_right_up_train,y_right_up_train)\n",
        "print(LDA_ovo_u_r .score(x_right_up_test,y_right_up_test))\n",
        "\n",
        "LDA_ovo_u_l = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_u_l.fit(x_left_up_train,y_left_up_train)\n",
        "print(LDA_ovo_u_l.score(x_left_up_test,y_left_up_test))\n",
        "\n",
        "LDA_ovo_d_r = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_d_r.fit(x_right_down_train,y_right_down_train)\n",
        "print(LDA_ovo_d_r.score(x_right_down_test,y_right_down_test))\n",
        "\n",
        "LDA_ovo_d_l = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_d_l.fit(x_left_down_train,y_left_down_train)\n",
        "print(LDA_ovo_d_l.score(x_left_down_test,y_left_down_test))\n",
        "\n",
        "LDA_ovo_r_l = LinearDiscriminantAnalysis()\n",
        "LDA_ovo_r_l.fit(x_left_right_train,y_left_right_train)\n",
        "print(LDA_ovo_r_l.score(x_left_right_test,y_left_right_test))\n",
        "\n",
        "LDA_ovm_u = LinearDiscriminantAnalysis()\n",
        "LDA_ovm_u.fit(x_up_train,y_up_train)\n",
        "print(LDA_ovm_u.score(x_up_test,y_up_test))\n",
        "\n",
        "LDA_ovm_r = LinearDiscriminantAnalysis()\n",
        "LDA_ovm_r.fit(x_right_train,y_right_trai76n)\n",
        "print(LDA_ovm_r.score(x_right_test,y_right_test))\n",
        "\n",
        "LDA_ovm_d = LinearDiscriminantAnalysis()\n",
        "LDA_ovm_d.fit(x_down_train,y_down_train)\n",
        "print(LDA_ovm_d.score(x_down_test,y_down_test))\n",
        "\n",
        "LDA_ovm_l = LinearDiscriminantAnalysis()\n",
        "LDA_ovm_l.fit(x_left_train,y_left_train)\n",
        "print(LDA_ovm_l.score(x_left_test,y_left_test))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdhKdrIgqCPT"
      },
      "source": [
        "Here I am doing the exact same thing except with an SVM classifier. The code has a nearly identical structure and therefore I will not comment it. SVM works by finding the hyperplane that seperates the 2 classes with the largest margin. This means that the distance between the points and seperating boundary is maximised which makes the classification more clear cut. This can be generalised to multiclass classification using OvO classification. I have specified in the initialisation of the model that probability=True which is an instruction to use platt scaling to generate the posterior probabilities and therefore the confidence of the model in all the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbYtSFhDpoSs"
      },
      "source": [
        "multiclass_SVM = svm.SVC(probability=True)\n",
        "multiclass_SVM.fit(x_csp_space_train,y_csp_space_train)\n",
        "print(multiclass_SVM.score(x_csp_space_test,y_csp_space_test))\n",
        "\n",
        "SVM_binary_search_v_h = svm.SVC(probability=True)\n",
        "SVM_binary_search_v_h.fit(x_vertical_horizontal_train,y_vertical_horizontal_train)\n",
        "print(SVM_binary_search_v_h.score(x_vertical_horizontal_test,y_vertical_horizontal_test))\n",
        "\n",
        "SVM_ovo_u_d = svm.SVC(probability=True)\n",
        "SVM_ovo_u_d.fit(x_up_down_train,y_up_down_train)\n",
        "print(SVM_ovo_u_d.score(x_up_down_test,y_up_down_test))\n",
        "\n",
        "SVM_ovo_u_r = svm.SVC(probability=True)\n",
        "SVM_ovo_u_r .fit(x_right_up_train,y_right_up_train)\n",
        "print(SVM_ovo_u_r .score(x_right_up_test,y_right_up_test))\n",
        "\n",
        "SVM_ovo_u_l = svm.SVC(probability=True)\n",
        "SVM_ovo_u_l.fit(x_left_up_train,y_left_up_train)\n",
        "print(SVM_ovo_u_l.score(x_left_up_test,y_left_up_test))\n",
        "\n",
        "SVM_ovo_d_r = svm.SVC(probability=True)\n",
        "SVM_ovo_d_r.fit(x_right_down_train,y_right_down_train)\n",
        "print(SVM_ovo_d_r.score(x_right_down_test,y_right_down_test))\n",
        "\n",
        "SVM_ovo_d_l = svm.SVC(probability=True)\n",
        "SVM_ovo_d_l.fit(x_left_down_train,y_left_down_train)\n",
        "print(SVM_ovo_d_l.score(x_left_down_test,y_left_down_test))\n",
        "\n",
        "SVM_ovo_r_l = svm.SVC(probability=True)\n",
        "SVM_ovo_r_l.fit(x_left_right_train,y_left_right_train)\n",
        "print(SVM_ovo_r_l.score(x_left_right_test,y_left_right_test))\n",
        "\n",
        "SVM_ovm_u = svm.SVC(probability=True)\n",
        "SVM_ovm_u.fit(x_up_train,y_up_train)\n",
        "print(SVM_ovm_u.score(x_up_test,y_up_test))\n",
        "\n",
        "SVM_ovm_r = svm.SVC(probability=True)\n",
        "SVM_ovm_r.fit(x_right_train,y_right_train)\n",
        "print(SVM_ovm_r.score(x_right_test,y_right_test))\n",
        "\n",
        "SVM_ovm_d = svm.SVC(probability=True)\n",
        "SVM_ovm_d.fit(x_down_train,y_down_train)\n",
        "print(SVM_ovm_d.score(x_down_test,y_down_test))\n",
        "\n",
        "SVM_ovm_l = svm.SVC(probability=True)\n",
        "SVM_ovm_l.fit(x_left_train,y_left_train)\n",
        "print(SVM_ovm_l.score(x_left_test,y_left_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv93tm28rV8r"
      },
      "source": [
        "The exact same thing is done with the Logistic Regresion. LR works by taking a linear combination of all the features, adding a bias and applying a sigmoid function to convert the value into a posterior probability automatically making the output the confidence of the model in one of the 2 classes. This can be generalised to multiclass classification using multinomial LR which essentially repeates the process for all the classes and applies a softmax transform rather than a sigmoid function. The weights and biases are optimised using gradient descent. lbfgs is a variant of this optimisation algorithm that is used on multinomial LR. The details of how it works is not relevant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gweV-dhOppR9"
      },
      "source": [
        "multiclass_LR = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "multiclass_LR.fit(x_csp_space_train,y_csp_space_train)\n",
        "print(multiclass_LR.score(x_csp_space_test,y_csp_space_test))\n",
        "\n",
        "LR_binary_search_v_h = LogisticRegression()\n",
        "LR_binary_search_v_h.fit(x_vertical_horizontal_train,y_vertical_horizontal_train)\n",
        "print(LR_binary_search_v_h.score(x_vertical_horizontal_test,y_vertical_horizontal_test))\n",
        "\n",
        "LR_ovo_u_d = LogisticRegression()\n",
        "LR_ovo_u_d.fit(x_up_down_train,y_up_down_train)\n",
        "print(LR_ovo_u_d.score(x_up_down_test,y_up_down_test))\n",
        "\n",
        "LR_ovo_u_r = LogisticRegression()\n",
        "LR_ovo_u_r .fit(x_right_up_train,y_right_up_train)\n",
        "print(LR_ovo_u_r .score(x_right_up_test,y_right_up_test))\n",
        "\n",
        "LR_ovo_u_l = LogisticRegression()\n",
        "LR_ovo_u_l.fit(x_left_up_train,y_left_up_train)\n",
        "print(LR_ovo_u_l.score(x_left_up_test,y_left_up_test))\n",
        "\n",
        "LR_ovo_d_r = LogisticRegression()\n",
        "LR_ovo_d_r.fit(x_right_down_train,y_right_down_train)\n",
        "print(LR_ovo_d_r.score(x_right_down_test,y_right_down_test))\n",
        "\n",
        "LR_ovo_d_l = LogisticRegression()\n",
        "LR_ovo_d_l.fit(x_left_down_train,y_left_down_train)\n",
        "print(LR_ovo_d_l.score(x_left_down_test,y_left_down_test))\n",
        "\n",
        "LR_ovo_r_l = LogisticRegression()\n",
        "LR_ovo_r_l.fit(x_left_right_train,y_left_right_train)\n",
        "print(LR_ovo_r_l.score(x_left_right_test,y_left_right_test))\n",
        "\n",
        "LR_ovm_u = LogisticRegression()\n",
        "LR_ovm_u.fit(x_up_train,y_up_train)\n",
        "print(LR_ovm_u.score(x_up_test,y_up_test))\n",
        "\n",
        "LR_ovm_r = LogisticRegression()\n",
        "LR_ovm_r.fit(x_right_train,y_right_train)\n",
        "print(LR_ovm_r.score(x_right_test,y_right_test))\n",
        "\n",
        "LR_ovm_d = LogisticRegression()\n",
        "LR_ovm_d.fit(x_down_train,y_down_train)\n",
        "print(LR_ovm_d.score(x_down_test,y_down_test))\n",
        "\n",
        "LR_ovm_l = LogisticRegression()\n",
        "LR_ovm_l.fit(x_left_train,y_left_train)\n",
        "print(LR_ovm_l.score(x_left_test,y_left_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDP57zsAz3oZ"
      },
      "source": [
        "This is the convolutional neural network. I will not comment every single line as the code for each layer is very repetitive so I will only do the first few."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIK_MOLVsV6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "609ff6c9-b127-4692-e908-d87fedda119a"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "#initialising neural network\n",
        "model = models.Sequential()\n",
        "#specifying topology of neural network\n",
        "#the first layer is 22 2 dimensional convolutions of size (11,1) with stride (1,1)\n",
        "# the output dimension will be (375-11+1,22-1+1,22)=(365,22,22)\n",
        "model.add(layers.Conv2D(22, (11,1), (1,1),input_shape=(375,22,1),padding=\"valid\"))\n",
        "#the next layer is a LeakyReLU activation where the contstant epsilon is 0.01\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "#the next layer is spatial dropout which whill ignore each of the 22 feature maps with probability 0.5\n",
        "model.add(layers.SpatialDropout2D(.5))\n",
        "#the next layer is 44 2 dimesnional convolutions of size (1,9) with stride (1,1)\n",
        "# the output dimension will be (365-1+1,22-8+1,44)=(365,14,44)\n",
        "model.add(layers.Conv2D(44, (1,9), (1,1),padding=\"valid\"))\n",
        "#the next layer is a batch normalisation layer\n",
        "model.add(layers.BatchNormalization())\n",
        "#the next layer is a LeakyReLU activation where the contstant epsilon is 0.01\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "#the next layer is a max pooling layer\n",
        "#the output dimension will be (365//2,14,44)=(182,14,44)\n",
        "model.add(layers.MaxPooling2D((2, 1),(2,1),padding=\"valid\"))\n",
        "#the next layer is 88 2 dimesnional convolutions of size (11,1) with stride (1,1)\n",
        "#output dimension is (182-11+1,14-1+1,88)=(172,14,88)\n",
        "model.add(layers.Conv2D(88, (11,1), (1,1),padding=\"valid\"))\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "model.add(layers.SpatialDropout2D(.5))\n",
        "model.add(layers.MaxPooling2D((2, 1),(2,1),padding=\"valid\"))\n",
        "model.add(layers.Conv2D(88, (11,1), (1,1),padding=\"valid\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "model.add(layers.SpatialDropout2D(.5))\n",
        "model.add(layers.MaxPooling2D((2, 1),(2,1),padding=\"valid\"))\n",
        "model.add(layers.Conv2D(176, (2,1), (1,1),padding=\"valid\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "model.add(layers.MaxPooling2D((2, 1),(2,1),padding=\"valid\"))\n",
        "model.add(layers.Conv2D(352, (2,1), (1,1),padding=\"valid\"))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.LeakyReLU(0.01))\n",
        "model.add(layers.MaxPooling2D((2, 1),(2,1),padding=\"valid\"))\n",
        "#This flattens the output of the previous layer into a 1d array that will now be fed into a dense layer\n",
        "model.add(layers.Flatten())\n",
        "#a dense layer with 4 outputs\n",
        "model.add(layers.Dense(4))\n",
        "#applying softmax transform to get probabilties\n",
        "model.add(layers.Softmax())\n",
        "#specifying training mechanism\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "#training neural network\n",
        "model.fit(np.array(x_train_nn_space),np.array(y_train_nn_space),epochs=500,validation_data=(np.array(x_test_nn_space), np.array(y_test_nn_space)))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 365, 22, 22)       264       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 365, 22, 22)       0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d (SpatialDr (None, 365, 22, 22)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 365, 14, 44)       8756      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 365, 14, 44)       176       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 365, 14, 44)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 182, 14, 44)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 172, 14, 88)       42680     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 172, 14, 88)       0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_1 (Spatial (None, 172, 14, 88)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 86, 14, 88)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 76, 14, 88)        85272     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 76, 14, 88)        352       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 76, 14, 88)        0         \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_2 (Spatial (None, 76, 14, 88)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 38, 14, 88)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 37, 14, 176)       31152     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 37, 14, 176)       704       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 37, 14, 176)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 18, 14, 176)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 17, 14, 352)       124256    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 17, 14, 352)       1408      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 17, 14, 352)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 8, 14, 352)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 39424)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4)                 157700    \n",
            "_________________________________________________________________\n",
            "softmax (Softmax)            (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 452,720\n",
            "Trainable params: 451,400\n",
            "Non-trainable params: 1,320\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcMV3JB8c882"
      },
      "source": [
        "The rest of the code is simply applying all the trained models and then taking their outputs and passing them through the relevant functions to obtain the final classification and therefore the overall accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-YEaVB7se-j"
      },
      "source": [
        "LDA_binary_outputs=[]\n",
        "LDA_ovo_outputs=[]\n",
        "LDA_ovm_outputs=[]\n",
        "LDA_multiclass_outputs=[]\n",
        "SVM_binary_outputs=[]\n",
        "SVM_ovo_outputs=[]\n",
        "SVM_ovm_outputs=[]\n",
        "SVM_multiclass_outputs=[]\n",
        "LR_binary_outputs=[]\n",
        "LR_ovo_outputs=[]\n",
        "LR_ovm_outputs=[]\n",
        "LR_multiclass_outputs=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5OkfOHq4Acw"
      },
      "source": [
        "for i in x_all_test:\n",
        "  current=feature_extraction(i,csp_multiclass)\n",
        "  LDA_multiclass_outputs.append(multiclass_LDA.predict([current]))\n",
        "  SVM_multiclass_outputs.append(multiclass_SVM.predict([current]))\n",
        "  LR_multiclass_outputs.append(multiclass_LR.predict([current]))\n",
        "print(accuracy(LDA_multiclass_outputs,y_all_test))\n",
        "print(accuracy(SVM_multiclass_outputs,y_all_test))\n",
        "print(accuracy(LR_multiclass_outputs,y_all_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPeilqxwyjO5"
      },
      "source": [
        "for i in x_all_test:\n",
        "  current_1=feature_extraction(i,csp_filters_up_right)\n",
        "  current_2=feature_extraction(i,csp_filters_up_down)\n",
        "  current_3=feature_extraction(i,csp_filters_up_left)\n",
        "  current_4=feature_extraction(i,csp_filters_down_right)\n",
        "  current_5=feature_extraction(i,csp_filters_right_left)\n",
        "  current_6=feature_extraction(i,csp_filters_down_left)\n",
        "  output_1=LDA_ovo_u_r.predict([current_1])\n",
        "  output_2=LDA_ovo_u_d.predict([current_2])\n",
        "  output_3=LDA_ovo_u_l.predict([current_3])\n",
        "  output_4=LDA_ovo_d_r.predict([current_4])\n",
        "  output_5=LDA_ovo_r_l.predict([current_5])\n",
        "  output_6=LDA_ovo_d_l.predict([current_6])\n",
        "  LDA_ovo_outputs.append(one_v_one([output_1,output_2,output_3,output_4,output_5,output_6]))\n",
        "  output_1=SVM_ovo_u_r.predict([current_1])\n",
        "  output_2=SVM_ovo_u_d.predict([current_2])\n",
        "  output_3=SVM_ovo_u_l.predict([current_3])\n",
        "  output_4=SVM_ovo_d_r.predict([current_4])\n",
        "  output_5=SVM_ovo_r_l.predict([current_5])\n",
        "  output_6=SVM_ovo_d_l.predict([current_6])\n",
        "  SVM_ovo_outputs.append(one_v_one([output_1,output_2,output_3,output_4,output_5,output_6]))\n",
        "  output_1=LR_ovo_u_r.predict([current_1])\n",
        "  output_2=LR_ovo_u_d.predict([current_2])\n",
        "  output_3=LR_ovo_u_l.predict([current_3])\n",
        "  output_4=LR_ovo_d_r.predict([current_4])\n",
        "  output_5=LR_ovo_r_l.predict([current_5])\n",
        "  output_6=LR_ovo_d_l.predict([current_6])\n",
        "  LR_ovo_outputs.append(one_v_one([output_1,output_2,output_3,output_4,output_5,output_6]))\n",
        "\n",
        "print(accuracy(LDA_ovo_outputs,y_all_test))\n",
        "print(accuracy(SVM_ovo_outputs,y_all_test))\n",
        "print(accuracy(LR_ovo_outputs,y_all_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C9X64we6rhh"
      },
      "source": [
        "for i in x_all_test:\n",
        "  current_1=feature_extraction(i,csp_filters_v_h)\n",
        "  current_2=feature_extraction(i,csp_filters_up_down)\n",
        "  current_3=feature_extraction(i,csp_filters_right_left)\n",
        "  output_1=LDA_binary_search_v_h.predict([current_1])\n",
        "  output_2=LDA_ovo_u_d.predict([current_2])\n",
        "  output_3=LDA_ovo_r_l.predict([current_3])\n",
        "  LDA_binary_outputs.append(binary_search([output_1,output_2,output_3]))\n",
        "  output_1=SVM_binary_search_v_h.predict([current_1])\n",
        "  output_2=SVM_ovo_u_d.predict([current_2])\n",
        "  output_3=SVM_ovo_r_l.predict([current_3])\n",
        "  SVM_binary_outputs.append(binary_search([output_1,output_2,output_3]))\n",
        "  output_1=LR_binary_search_v_h.predict([current_1])\n",
        "  output_2=LR_ovo_u_d.predict([current_2])\n",
        "  output_3=LR_ovo_r_l.predict([current_3])\n",
        "  LR_binary_outputs.append(binary_search([output_1,output_2,output_3]))\n",
        "print(accuracy(LDA_binary_outputs,y_all_test))\n",
        "print(accuracy(SVM_binary_outputs,y_all_test))\n",
        "print(accuracy(LR_binary_outputs,y_all_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLyF3oMu68yC"
      },
      "source": [
        "for i in x_all_test:\n",
        "  current_1=feature_extraction(i,csp_filters_up)\n",
        "  current_2=feature_extraction(i,csp_filters_down)\n",
        "  current_3=feature_extraction(i,csp_filters_left)\n",
        "  current_4=feature_extraction(i,csp_filters_right)\n",
        "  output_1=(LDA_ovm_u.predict_proba([current]))[0][0]\n",
        "  output_2=(LDA_ovm_d.predict_proba([current]))[0][0]\n",
        "  output_3=(LDA_ovm_l.predict_proba([current]))[0][0]\n",
        "  output_4=(LDA_ovm_r.predict_proba([current]))[0][0]\n",
        "  LDA_ovm_outputs.append(one_v_many([output_1,output_4,output_2,output_3]))\n",
        "  output_1=(SVM_ovm_u.predict_proba([current]))[0][0]\n",
        "  output_2=(SVM_ovm_d.predict_proba([current]))[0][0]\n",
        "  output_3=(SVM_ovm_l.predict_proba([current]))[0][0]\n",
        "  output_4=(SVM_ovm_r.predict_proba([current]))[0][0]\n",
        "  SVM_ovm_outputs.append(one_v_many([output_1,output_4,output_2,output_3]))\n",
        "  output_1=(LR_ovm_u.predict_proba([current]))[0][0]\n",
        "  output_2=(LR_ovm_d.predict_proba([current]))[0][0]\n",
        "  output_3=(LR_ovm_l.predict_proba([current]))[0][0]\n",
        "  output_4=(LR_ovm_r.predict_proba([current]))[0][0]\n",
        "  LR_ovm_outputs.append(one_v_many([output_1,output_4,output_2,output_3]))\n",
        "print(accuracy(LDA_ovm_outputs,y_all_test))\n",
        "print(accuracy(SVM_ovm_outputs,y_all_test))\n",
        "print(accuracy(LR_ovm_outputs,y_all_test))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}